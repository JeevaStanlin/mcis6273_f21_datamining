{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# MCIS6273 Data Mining (Prof. Maull) / Fall 2021 / HW2\n", "\n", "**This assignment is worth up to 20 POINTS to your grade total if you complete it on time.**\n", "\n", "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n", "|:---------------:|:--------:|:---------------:|\n", "| 20 | Wednesday, October 20 @ Midnight | _up to_ 24 hours |\n", "\n", "\n", "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n", "\n", "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n", "\n", "## OBJECTIVES\n", "* Learn about the new data economy through Ocean Protocol and how Blockchain is fueling innovation in the data space\n", "\n", "* Continue practicing exploratory data analysis and visualization\n", "\n", "* Perform a clustering analysis using k-means\n", "\n", "## WHAT TO TURN IN\n", "You are being encouraged to turn the assignment in using the provided\n", "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n", "`homework/hw2`.   Put all of your files in that directory. \n", "\n", "Then zip that directory,\n", "rename it with your name as the first part of the filename (e.g. `maull_hw2_files.tar.gz`), then\n", "download it to your local machine, then upload the `.tar.gz` to Blackboard.\n", "\n", "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n", "on the basics of using in Linux.\n", "\n", "If you choose not to use the provided notebook, you will still need to turn in a\n", "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n", "this homework.\n", "\n", "\n", "## ASSIGNMENT TASKS\n", "### (20%) Learn about the new data economy through Ocean Protocol and how Blockchain is fueling innovation in the data space \n", "\n", "You have no doubt heard the phrase \"Data is the new oil\", \n", "which goes back to 2006, when British data scientist and \n", "mathematician Clive Humby first coined the phrase.\n", "\n", "For an interesing chronology of the phrase over the past 15 \n", "years or so, please read the follow Medium.com post:\n", "\n", "* [\u201cData is the New Oil\u201d \u2014 A Ludicrous Proposition by Michael Haupt](https://medium.com/project-2030/data-is-the-new-oil-a-ludicrous-proposition-1d91bba4f294)\n", "\n", "You can decide for yourself, after some contemplation, whether\n", "the \"privatization\" of data is right or wrong, and what\n", "one might do about it if they choose to.         \n", "\n", "Nonetheless, the parallels to oil, and the oil boom at \n", "the turn of the 20th century -- which profoundly changed \n", "our relationship with energy and quite literally fueled \n", "innovation on a scale that humanity has never seen -- \n", "are interesting. Data, however, is not a finite \n", "resource, and the \"ownership\" of data is quite different \n", "than that of oil (however controversial that may be). \n", "What should be obvious is that producing data and \n", "\"enriching it\" (adding \"value\") are two different \n", "things, and \"value\" is a term that requires a context.\n", "\n", "Nonetheless, we are at an interesting point in the big \n", "data, data mining and data science conversation about \n", "what to do with and how to treat all the data now \n", "being produce, particularly as the kind of data that is \n", "becoming interesting to companies is increasingly \n", "personal (i.e. personal health data) and quite possibly \n", "being put to use in contexts we have no transparency or \n", "control over.\n", "\n", "Blockchain technologies are rapidly become the _de \n", "facto_ platforms for data-related transactions, and \n", "interesting work is being done to use these distributed \n", "digital ledgers as data exchange platforms.  Even more\n", "work is being done to bring \"data \n", "ownership\" to a new level on digital marketplaces and\n", "the \"Internet of Value\" or IoV (see also [this 2017 writeup on the IoV by Ripple](https://ripple.com/insights/the-internet-of-value-what-it-means-and-how-it-benefits-everyone/))\n", "building the \"new data economy\".\n", "\n", "You will be listening to a podcast on this subject involving\n", "the [Ocean Protocol](https://oceanprotocol.com) and its\n", "founder Bruce Pon.  You will listen the almost 43 minute\n", "podcast from [The Crypto Conversation Podcast](https://bravenewcoin.com/insights/podcasts) on \n", "[Brave New Coin](https://bravenewcoin.com).\n", "\n", "* \"Data is the new oil - Ocean Protocol is building the data economy\" - [October 28, 2020 Podcast with Bruce Pon](https://bravenewcoin.com/insights/podcasts/data-is-the-new-oil-ocean-protocol-is-building-the-data-economy);\n", "* download the [mp3 file directly](https://traffic.libsyn.com/secure/thecryptoconversation/bruce_ocean_final.mp3).\n", "\n", "&#167;  Listen to the entire podcast and answer the questions below:\n", "\n", "1. What was the original idea that Pon tried \n", "in 2013? \n", "\n", "1. Who (or what) did Pon say was the \n", "\"consumer\" of Blockchain and big data?  How \n", "does this relate to \"data mining\"?\n", "\n", "1. Who does Pon say are the gatekeepers of the \n", "current \"data economy\"?  How does Ocean \n", "Protocol address this?\n", "\n", "1. How does Pon suggest data might be \n", "\"securitized\" over Ocean Protocol?  Use the \n", "example he gives directly.\n", "\n", "1. What are the four landscape users (or \n", "targets) of Ocean Protocol?\n", "\n", "1. What example does he give of a liquid dataset \n", "that might have tremendous value in a \n", "variety of current and future contexts relating to our world?\n", "\n", "1. What does Pon predict the crypto \n", "marketplace will look like in 10 years?\n", "\n", "1. After consuming this podcast, please list \n", "your reaction to it in a short paragraph of at a minimum of three \n", "**complete** sentences.  Address what you \n", "already know about crypto and the data economy \n", "as well as the things that you learned that \n", "were new knowledge.  You may also address \n", "whether you agree or disagree with Pon's \n", "positions, especially how it relates to Ocean \n", "Protocol and cryptocurrencies, in general, \n", "becoming a great equalizer.  You can also include\n", "any personal experiences you have with \n", "the technology.\n", "\n", "\n", "\n", "### (25%) Continue practicing exploratory data analysis and visualization \n", "\n", "In the last HW we explore some of the basic features of Pandas with graphic and data selection.\n", "This time we're going to go a but deeper into Pandas ans learn about MultIndices and grouping\n", "data in interesting and useful ways.\n", "\n", "Power weightlifting (powerlifting) is an international sport that invites advanced amateurs and professionals alike. Fortunately, there are datasets for the multitude of powerlifting competitions around the world, and they are openly available for curious data scientists like ourselves who would like to ask interesting questions and find interesting relationships in the data. Whether you're into the sport or not, I think there are a variety of interesting phenomenon in the data that make it both tractable and interesting from just a data perspective.\n", "\n", "**DATA**\n", "\n", "OpenPowerLifting.org is a large set of data for a multitude of data related to powerlifting competitions around the world. The core data live at the following open source repository on gitlab.com/openpowelifting.\n", "\n", "For the curious, there are a number of analyses that have already been performed on the data in a number of interesting ways. Please visit this page to further fill your intrigue.\n", "\n", "Though the full dataset is available to us and will be used in the next part of the assignment, we want to get a little practice getting data from websites that require some HTML parsing and navigation. Unlike the last time where we used APIs to get data, we're going to build a dataset en mass from the CSV data on Gitlab. Remarkably, while this technique may seem antiquated, you will find many datasets are just sitting on servers as text files that will require something similar to be accomplished. We're unfortunately not yet in a data environment where the most interesting data you want is easily obtained or accessible behind APIs. Often the resources to do so are beyond the capabilities of the data providers, though things are getting better each year with new tools and data access platforms.\n", "\n", "We're going to use Python and the Beautiful Soup library to build a random dataset of just 2019 data by directly navigating the HTML of the Gitlab repository. It will be noted that Gitlab does have an API, and it would be the preferred mechanism if we were to do this exercise with APIs like the last assignment.\n", "\n", "One of the things that we will learn from the data is that the majority of it are\n", "interesting over several dimensions.  There are the years of competition, the sex of the competitors,\n", "the age the competitors, country of origin, among other things.  With denser data like these, we want\n", "to understand some of the underlying groupings for easier access to the data.  For example,\n", "one might want to understand how groupings by year and age bear out on the data to explore questions\n", "like \"Has the number of competitors over 40 increased over the years?\"  This might be an interesting\n", "question to ask to explore if powerlifters continue to compete as they age since the sport is very\n", "difficult on one's body and requires intense continuous training to stay competitive.\n", "\n", "Some questions like these are also very useful to explore visually, so we'll dive into a few more\n", "graphical techniques to get at these answers and more.  We're going to end up with a DataFrame\n", "that will group our data by year, age class and sex, so we can see some of the interesting\n", "annual trends along each of these dimensions within the last two decades.\n", "\n", "&#167;  **BUILD THE DATASET**\n", "\n", "We've learned CSV is common file format for data and we will be working the files large text files in Gitlab to do the work we need. The task is to explore the repository and build up a dataset of 15 random lifting meets from 2019 using BeautifulSoup and the tools in Pandas to put these datasets together.\n", "\n", "This technique is often known as \"crawling\" and is consider by some to be a flagrant violation of good web etiquette. However, the technique is still often the only way to obtain data en mass from a single source. If this were an FTP server, the same pattern could be applied and would not be considered unusual to do so. Of course, you must use this with caution, as it can result in IP throttling and IP blocking, so please use it within the licensing terms of both the data and website you are obtaining data from. Good web citizens restore trust in providers and administrators alike, so throttling yourself after your own requests with code like time.sleep(2) (which will pause your code for 2 seconds), will show that you can behave responsibly.\n", "\n", "Once you have loaded all the files, please re-index the rows using the Dataframe.reset_index() method.\n", "\n", "\n", "&#167;  **FILTER AND EXPLORE THE DATA**\n", "\n", "Let's first get a feel for the data and filter it down.  One of the main\n", "difficulties in dealing with large user-contributed data sets like these\n", "are _data consistency_ and _data quality_.  _Data consistency_ refers to\n", "how data is represented over time.  We can see how this becomes an issue\n", "when we look at the `Division` column of the dataset.  We can see with a\n", "relatively untrained eye to the data, that something is very wrong with\n", "the consistency &mdash; there are over 1300 Division designations!  When\n", "you look at it more closely, there are groupings that overlap.  For\n", "example, you will see `Masters 45-49` and `Masters 40-49` when you\n", "perform a `.value_counts()` on the `Divisions` column of the data (see\n", "supplemental notebook).  What is the difference between these two since\n", "they obviously overlap?\n", "\n", "Filter the data down to a smaller subset of the data using\n", "[`dropna()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html).\n", "Specifically, drop all rows of data missing `Age`\n", "and `Division` data (e.g. they have `NaN` values).  You will be able to do this by passing a\n", "`subset=` parameter into `dropna()`.\n", "\n", "You should now have fewer than 200K data points, leading us to the second\n", "problem of _data quality_.  When we look at the original data file, we have\n", "over 400K data points, yet after filtering for missing values, we end up\n", "with over 50% reduction in the data!  This is actually a reasonable preservation\n", "of the original data set given that we `NaN` values from two different columns.\n", "In general, it isn't a bad idea when working with your data to develop an\n", "understanding of the holes in it.  What if we reduced our data 90% just on\n", "`Age` indicating it was not recorded consistently over time?  This would indeed\n", "limit what we could ask of the data in analyses requiring age data.  Many of\n", "the tools in Pandas will ignore `NaN` data, but some required you to send\n", "specific instructions to the tool on what to do.  Better to get ahead of things\n", "now.\n", "\n", "In your notebook **you must include the following** to be considered a correct answer:\n", "\n", "* correct use of `dropna()`,\n", "* output of the original dimensions of the data using `DataFrame.shape` or\n", "  something similar that shows the original and new dimensions of the data.\n", "\n", "\n", "&#167;  **CLEAN THE DATA**\n", "\n", "One of the major issues with any data set when importing into Pandas is the\n", "that Pandas tries to infer the data types so that when you compare data\n", "you are comparing data that _can_ be compared (i.e. you cannot  compare\n", "strings with integers).  One area that you'll need to be mindful\n", "of is with dates.  For example, if you perform a `df.Date.head()` the\n", "data is of type `object`.  We want it to be `Datetime` so we can benefit\n", "from the many useful features in Pandas to manipulate dates.\n", "\n", "Doing this is straightforward with the [`pandas.datetime()`](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html#pandas.to_datetime) method.\n", "Your notebook must show:\n", "\n", "* use of `pandas.datetime` to convert the `Date` column\n", "  of your data to `datetime` (hint: use `loc` to set the data),\n", "* that the column has changed by using `DataFrame.dtype` or alternatively\n", "  use `DataFrame.head()` which will show the data type in its output.\n", "\n", "**NOTE**: you may need to use the `errors` parameter of `to_datetime` to\n", "handle any issues you may encounter.\n", "\n", "\n", "&#167;  **GROUP THE DATA**\n", "\n", "Pandas provides superior capabilities to slice and group data.  We would like\n", "to build a dataset that is composed of all remaining data from\n", "1999 to 2018 that restricts age to those 21 and older.\n", "\n", "Like the last homework, doing this is simple with [`Dataframe.query()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html#pandas.DataFrame.query).  You will notice that a\n", "query over the `Date` column can be done naturally by just saying\n", "`Date > a_year` since it is a `datetime` object.  Pandas does the inferential\n", "magic for you!\n", "\n", "Next add to your query by grouping the data by `Date`, `AgeClass`\n", "and `Sex`.   You will need to use the [`DataFrame.groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby)\n", "method to accomplish this.  **A hint for grouping by year**: if you do not restrict\n", "the data in the `Date` column Pandas will naturally group by the\n", "full date meaning that each _day_ will be grouped leading to the\n", "wrong result.  You can convert a date to a year for the purposes of\n", "grouping by year using `dt.to_period('Y')`.  Please see the documentation for\n", "[`Series.dt.to_period()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.to_period.html).\n", "\n", "You will pass to the first parameter of `DataFrame.groupby()` the list of\n", "the grouping in order of grouping, outer group first.  Thus, `groupby(['Sex', 'AgeClass`])\n", "will return the MultiIndex DataFrame with `Sex` as the outermost group\n", "and `AgeClass` the inner group.  See the supplemental notebook for some\n", "more clues.\n", "\n", "Your notebook must show:\n", "\n", "* use of `query()` to restrict your data to competitors 21 and older\n", "  in competitions from 1999 to 2018,\n", "* use of `groupby` showing the **`mean()`** values for each column with\n", "  groups being (in order) `Date (year)`, `AgeClass` and `Sex`.\n", "\n", "\n", "&#167;  **VISUALIZE**\n", "\n", "Now that we have the data segmented the way we'd like, let's visualize it\n", "in some interesting way.\n", "\n", "With powerlifting there are a number of ways to express the _strength_ of a\n", "competitor. There is _raw_ strength, meaning how much total weight was lifted\n", "on a given lift, and there is _relative_ strength.  It is not fair to compare\n", "the raw lift of a 100lb 16 year old teenage to that of a 35 year old 300lb adult.\n", "\n", "The 35 year old might lift ten times the weight yet the 16 year old may be _relatively_\n", "stronger, but how would we compare their\n", "_relative_ strengths?  Many competitors will be able to lift between 2 and 7 times\n", "their body weight depending on the lift, so we might expect a 100lb powerlifter\n", "to perhaps perform a 200lb bench press and maybe a 300lb squat, both impressive\n", "for their weight.  To deal with comparing _strength_ across age and weight variables\n", "a number of methods have been developed to create fair and accurate measures of\n", "_relative strength_.  The OpenPowelifting dataset includes three such measures:\n", "_Wilks_, _McCulloch_ and _Glossbrenner_, which give a numeric assignment of relative\n", "strength which factor age and weight into the computation.  Exploring the details\n", "of each of these methods is beyond the scope of this homework, but the curious can\n", "learn more on the variety of sites which calculate these statistics.\n", "\n", "We will restrict our interest to the _Glossbrenner_ score, which takes into account\n", "age and weight to compute a normalized weight value.  Consider three competitors,\n", "all 29 year olds, with one male and one female weighting 131.84lbs and the last male\n", "weighting 263.67 pounds.  Assume they all lift 639.33 pounds total.  The _Glossbrenner_ score\n", "takes into account the age and weights and produces a relative score with the\n", "following:\n", "\n", "| competitor | sex | age (lbs) | weight (lbs) | lift (lbs) | score | $\\gamma$-coefficient | $\\gamma_{age}$-coefficient\n", "|-----------:|:---:|:---:|:------:|:-----:|:-----:|:--------------------:|:----:|\n", "| 1 | F | 29 |  131.84 | 639.33 | 287.187 | 0.9903 | 1 |\n", "| 2 | M | 29 |  131.84 | 639.33 | 242.3095 | 0.83555 | 1 |\n", "| 3 | M | 29 |  263.67 | 639.33 | 159.906 | 0.5514 | 1 |\n", "| 4 | M | 49 | 263.67 | 639.33 | 218.951 | 0.5514 | 1.113 |\n", "| 5 | F | 49 | 263.67 | 639.33 | 177.9754 | 0.67835 | 1.113 |\n", "\n", "\n", "The _Glossbrenner_ score is in the _score_ column and the $\\gamma$-coefficient is\n", "the constant calculated by the method.  The $\\gamma_{age}$-coefficient is computed\n", "constant which the method factors in for the relative impact age has on the\n", "competitor.  Thus, the _Glossbrenner_ score $\\Gamma$ is:\n", "\n", "$$\n", "\\Gamma(age, sex) = \\gamma{\\text{-coefficient}}_{age,sex} \\times \\gamma_{age} \\times weight\n", "$$\n", "\n", "If you perform the score calculation on the Openpowerlifting data, you'll notice the\n", "some values are off by a small amount and this is likely due to the _Glossbrenner_ constants\n", "used, which have varied over time.\n", "\n", "Now that we have that out of the way, let's visualize some data.  Specifically, we'd like to\n", "plot the _Glossbrenner_ score for the last 20 years over time.  Are the scores going up, down\n", "or staying the same?  One could expect any of these scenarios to occur, so let's dive in.\n", "\n", "What we want to produce are two _area_ plots of the annual _mean Glossbrenner_ score\n", "from 1999 to 2018 for all age groups, one plot for males and the other for females as putting them all on\n", "one graph would most certainly be information overload.  To do this we will need to\n", "slice the data in a way that makes a multi-index grouped by year, age group and sex.\n", "You will effectively use the data from the prior part and make a visual of it.\n", "\n", "Your area plot code will be invoked by:\n", "\n", "```python\n", "\n", "DataFrame.plot.area()\n", "\n", "```\n", "\n", "You may optionally pass in the `figsize=(15,7)` (or whatever dimensions you'd like)\n", "to stretch the data out a bit so you can visually see what is going on, since the\n", "legend may get in the way of viewing the data.\n", "\n", "Your plot will look something like this:\n", "\n", "![](./sample_area.png)\n", "\n", "Please see the [`DataFrame.plot.area()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.area.html#pandas.DataFrame.plot.area) method for full information on\n", "the area plots.\n", "\n", "**A final important note**: You will need to use [`droplevel()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.droplevel.html) and\n", "[`unstack()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unstack.html)\n", "in order to prepare your DataFrame for final presentation.  Basically,\n", "you'll need to drop the `Sex` level of your index (level 2) and immediately\n", "before you plot the area plot you will use `unstack()`.\n", "\n", "Your notebook must show:\n", "\n", "* an area plot showing the male data grouped by age and year, that\n", "  is the $x$-axis will show the year and the $y$-axis the _Glossbrenner_ score,\n", "* an area plot showing the female data grouped by age and year, that\n", "  is the $x$-axis will show the year and the $y$-axis the _Glossbrenner_ score.\n", "\n", "You must also answer the following questions in your notebook:\n", "\n", "* What's the general trend you see in the area plots?  Your answer can\n", "  be in one or two sentences.\n", "\n", "\n", "\n", "### (50%) Perform a clustering analysis using k-means \n", "\n", "The simplicity and power of k-means algorithm makes it one of the best to start with\n", "when performing _unsupervised learning_ &mdash; that is the class labels of your\n", "data are not known _a priori_ and you that will not be training the algorithm\n", "on labeled data.  While this is a powerful and oft useful technique, use it with\n", "care as the initial conditions of the algorithm do not guarantee a global maximum\n", "and as such, running the algorithm with a number of initialization points will\n", "produce better and more reliable results.\n", "\n", "Continuing with our OpenPowerlifting data, we're going to do some exploratory data\n", "analysis to examine this dataset in some interesting ways using unsupervised learning,\n", "namely clustering.  The original dataset has over 1 million data points, but in order\n", "to get a good idea of what's in it, we will not need to go back through the entire\n", "dataset, and in fact, we will restrict the focus of our energy on just the last 2 decades\n", "from 1999.\n", "\n", "**REMEMBER TO MAKE SURE TO SHOW ALL YOUR WORK IN THE NOTEBOOK SO YOU CAN RECEIVE PARTIAL CREDIT WHERE APPROPRIATE!**\n", "\n", "&#167;  **PREPARE FOR CLUSTERING**\n", "\n", "You will need to complete part 1 of this homework to filter the data to the necessary\n", "subset for this part.  As we talked about in lecture, the subset of features will just\n", "be the following:\n", "\n", "  ```python\n", "  features = [\n", "      'Sex',\n", "      'Age',\n", "      'BodyweightKg',\n", "      'Best3SquatKg',\n", "      'Best3BenchKg',\n", "      'Best3DeadliftKg',\n", "      'TotalKg',\n", "      'Date'\n", "  ]\n", "  ```\n", "\n", "Final preparation for clustering will require you to turn all of\n", "_categorical_ variables into _numeric_ one's.  One way to do this\n", "from directly within Pandas is to use `Pandas.get_dummies(your_dataframe)`.\n", "You can also study the\n", "`sklearn.preprocessing.OrdinalEncoder()` which will do something\n", "very similar.  Either way, with the reduced set of features above, the only\n", "categorical variable will be `Sex` as all the others should already\n", "be numerical features.\n", "\n", "In your notebook, you should show:\n", "\n", "* clearly how many features are now in your dataframe?\n", "\n", "\n", "&#167;  **PERFORM SILHOUETTE ANALYSIS**\n", "\n", "In class we talked about the fact that the $k$ number of clusters needs to be\n", "determined _a priori_ &mdash; that is you will need to know how many clusters beforehand to\n", "run the algorithm.  To find the optimal $k$, we will use a method called the _silhouette score_.\n", "\n", "Adapt the following code to compute the silhouette scores on *only* the dataset filtered by\n", "the features from the prior step.\n", "\n", "```python\n", "  from sklearn.cluster import KMeans\n", "  from sklearn.metrics import silhouette_score\n", "\n", "  Sum_of_squared_distances = []\n", "  K = range(2, 15)\n", "  for k in K:\n", "      km = KMeans(n_clusters=k, n_init=20)\n", "      km = km.fit(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS)\n", "      Sum_of_squared_distances.append(km.inertia_)\n", "\n", "      silh_score = silhouette_score(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS, km.labels_)\n", "      print(\"k = {} | silhouette_score = {}\".format(k, silh_score))\n", "```\n", "\n", "The largest score is typically the $k$ you go with.  If $k=2$ is your largest\n", "score, we will ignore and use the next best score since 2 clusters is not usually an\n", "interesting number of clusters when dealing with a large set of data points.\n", "\n", "Your notebook must show and answer the following:\n", "\n", "* What is the optimal $k$ according the silhouette score?\n", "* What else is interesting about the scores?\n", "\n", "\n", "&#167;  **CLUSTER INTERPRETATION**\n", "\n", "Now that you have clusters and optimal cluster, let's find out the characteristics of\n", "the features that dominate them.\n", "\n", "Note that the k-means algorithm returns the cluster centers\n", "for each cluster, hence in that center each feature value\n", "is the _representative feature value_ for that cluster.\n", "For example, the `TotalKg` would be the representative `TotalKg` for\n", "that cluster.\n", "\n", "Using the optimal cluster size from the silhouette score in the prior\n", "section, please use adapt the following code to determine the cluster\n", "characteristics.\n", "\n", "```python\n", "    optimal_k = THE_OPTIMAL_SILH_K\n", "\n", "    km = KMeans(n_clusters=optimal_k, n_init=150)\n", "    km = km.fit(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS)\n", "\n", "    for i in range(0, optimal_k):\n", "        l = list(zip(YOUR_OPENPOWERLIFTING_DATAFRAME_WITH_DUMMY_VARS.columns, \\\n", "                    km.cluster_centers_[i]))\n", "        l.sort(key=lambda x: x[1], reverse=True)\n", "\n", "        print('CLUSTER : {}\\n'.format(i))\n", "        for attr, val in l[:]:\n", "          print('\\t{} : {}\\n'.format(attr, val))\n", "```\n", "\n", "Your notebook must show and answer the following:\n", "\n", "* for each cluster, describe in real words what the cluster centers are telling\n", "  you about the representative of that cluster.  For example, your answer might\n", "  look like: \"for cluster 1, the representative for that cluster is a 24.7 year\n", "  old female, with an average `Best3SquatKg` of 121 and a `TotalKg` of 721\",\n", "* show the output of the cluster centers above.\n", "\n", "**NOTE**: The order of the features in `km.cluster_centers_` are the same order\n", "as they exist in the DataFrame.\n", "\n", "\n", "\n"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [default]", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "12px", "width": "252px"}, "navigate_menu": true, "number_sections": false, "sideBar": true, "threshold": "1", "toc_cell": false, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 0}
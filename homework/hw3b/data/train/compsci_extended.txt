Algorithms are usually published in scholarly articles, especially in the computational sciences and related disciplines. The ability to automatically find and extract these algorithms in this increasingly vast collection of scholarly digital documents would enable algorithm indexing, searching, discovery, and analysis. Recently, AlgorithmSeer, a search engine for algorithms, has been investigated as part of CiteSeer' with the intent of providing a large algorithm database. Currently, over 200,000 algorithms have been extracted from over 2 million scholarly documents. This paper proposes a novel set of scalable techniques used by AlgorithmSeer to identify and extract algorithm representations in a heterogeneous pool of scholarly documents. Specifically, hybrid machine learning approaches are proposed to discover algorithm representations. Then, techniques to extract textual metadata for each algorithm are discussed. Finally, a demonstration version of AlgorithmSeer that is built on Solr/Lucene open source indexing and search system is presented.
Coal mine safety is crucial to the healthy and sustainable development of the coal industry, and coal mine flood is a major hidden danger of coal mine accidents. Therefore, the processing of coal mine water source data is of great significance to prevent mine water inrush accidents. In this experiment, the water source data were obtained by laser induced fluorescence technology with the assistance of laser. The water sample data information was preprocessed by standard normal variable transformation (SNV) and multiple scattering correction (MSC), and then the principal component analysis (PCA) was used to reduce the dimension of the data and ensure the information characteristics of the original data unchanged. In order to identify the water inrush type of coal mine water source, the sparrow search algorithm (SSA) is used to optimize the BP neural network in this study. This is because the SSA algorithm has the advantages of strong optimization ability and fast convergence rate compared with particle swarm optimization (PSO) and other optimization algorithms. Experiments show that under the premise of SNV pretreatment, the R2 of SSA-BP model is infinitely close to 1, MRE is 0.0017, RMSE is 0.0001, the R2 of PSO-BP model is 0.9995, MRE is 0.0026, RMSE is 0.0019, the R2 of BP model is 0.9983, MRE is 0.0140, RMSE is 0.0075. Therefore, SSA-BP model is more suitable for the classification of coal mine water sources.
Mining frequent simultaneous attribute co-variations in numerical databases is also called frequent gradual pattern problem. Few efficient algorithms for automatically extracting such patterns have been reported in the literature. Their main difference resides in the variation semantics used. However in applications with temporal order relations, those algorithms fail to generate correct frequent gradual patterns as they do not take this temporal constraint into account in the mining process. In this paper, we propose an approach for extracting frequent gradual patterns for which the ordering of supporting objects matches the temporal order. This approach considerably reduces the number of gradual patterns within an ordered data set. The experimental results show the benefits of our approach.
Exploring software testing possibilities at an early software life cycle is increasingly necessary to avoid the propagation of defects to the subsequent phases. This requirement demands technique that can generate automated test cases at the initial phases of software development. Thus, we propose a novel framework for automated test data generation using formal specifications written in object constraint language (OCL). We also defined a novel fitness function named exit-predicate-wise branch coverage (EPWBC) to evaluate the generated test data. Another focus of the proposed approach is to optimise the test case generation process by applying, harmony search (HS) algorithm. The experimental results indicate that the proposed framework outperforms the other OCL-based test case generation techniques. Furthermore, it has been inferred that OCL based testing adopting HS algorithm forms an excellent combination to produce more test coverage and an optimal test suite thereby improving the quality of a system.
Automation of test data generation is of prime importance in software testing because of the high cost and time incurred in manual testing. This paper proposes an Improved Crow Search Algorithm (ICSA) to automate the generation of test suites using the concept of mutation testing by simulating the intelligent behaviour of crows and Cauchy distribution. The Crow Search Algorithm suffers from the problem of search solutions getting trapped into the local search. The ICSA attempts to enhance the exploration capabilities of the metaheuristic algorithm by utilizing the concept of Cauchy random number. The concept of Mutation Sensitivity Testing has been used for defining the fitness function for the search based approach. The fitness function used, aids in finding optimal test suite which can achieve high detection score for the Program Under Test. The empirical evaluation of the proposed approach with other popular meta-heuristics, prove the effectiveness of ICSA for test suite generation using the concepts of mutation testing.
Our new algorithm and data structure, pyramid search (PS) and skip ring, were created with the help of circular linked list and skip list algorithms and data structures. In circular linked list, operations were performed on a single circular list. Our new data structure consists of circular linked lists formed in layers which were linked in a pyramid way. Time complexity of searching, insertion and deletion algorithms equal to [Formula: see text] (lg[Formula: see text]) in an [Formula: see text]-element skip ring data structure. Therefore, skip ring data structure is employed more effectively ([Formula: see text](lg[Formula: see text])) in circumstances where circular linked lists ([Formula: see text]) are used. The priority is determined based on the searching frequency in PS which was developed in this study. Thus, the time complexity of searching is almost [Formula: see text](1) for [Formula: see text] records data set. In this paper, the applications of searching algorithms like linear search (LS), binary search (BS) and PS were realized and the obtained results were compared. The obtained results demonstrated that the PS algorithm is superior to the BS algorithm.
The existing keyword-based search algorithms based on streaming data are hard to meet the needs of users for real-time data processing. To solve this problem, multi-keyword parallel search algorithm for streaming RDF data (MPSASR) proposed in this paper combines the Spark and Redis frameworks to construct query subgraphs integrated with ontology based on the query keywords in real time. Associated with scoring function, regarding the high-priority query subgraph as a guide, parallel search is performed in the instance data, and finally the Top-k query results are returned. Of course, our algorithm uses a hash compression algorithm to compress RDF data, which reduces the space required. Moreover, our algorithm makes full use of historical data and effectively speeds up search efficiency. Our algorithm is experimentally verified to have great advantages in real-time search, response time, and search effects.
This paper presents a multi-objective retinal blood vessels localization approach based on flower pollination search algorithm (FPSA) and pattern search (PS) algorithm. FPSA is a new evolutionary algorithm based on the flower pollination process of flowering plants. The proposed multi-objective fitness function uses the flower pollination search algorithm (FPSA) that searches for the optimal clustering of the given retinal image into compact clusters under some constraints. Pattern search (PS) as local search method is then applied to further enhance the segmentation results using another objective function based on shape features. The proposed approach for retinal blood vessels localization is applied on public database namely DRIVE data set. Results demonstrate that the performance of the proposed approach is comparable with state of the art techniques in terms of accuracy, sensitivity, and specificity with many extendable features.
Data clustering is a popular analysis tool for data statistics in many fields such as pattern recognition, data mining, machine learning, image analysis, and bioinformatics. The aim of data clustering is to represent large datasets by a fewer number of prototypes or clusters, which brings simplicity in modeling data and thus plays a central role in the process of knowledge discovery and data mining. In this paper, a novel data clustering algorithm based on modified Gravitational Search Algorithm is proposed, which is called Bird Flock Gravitational Search Algorithm (BFGSA). The BFGSA introduces a new mechanism into GSA to add diversity, a mechanism which is inspired by the collective response behavior of birds. This mechanism performs its diversity enhancement through three main steps including initialization, identification of the nearest neighbors, and orientation change. The initialization is to generate candidate populations for the second steps and the orientation change updates the position of objects based on the nearest neighbors. Due to the collective response mechanism, the BFGSA explores a wider range of the search space and thus escapes suboptimal solutions. The performance of the proposed algorithm is evaluated through 13 real benchmark datasets from the well-known UCI Machine Learning Repository. Its performance is compared with the standard GSA, the Artificial Bee Colony (ABC), the Particle Swarm Optimization (PSO), the Firefly Algorithm (FA), K-means, and other four clustering algorithms from the literature. The simulation results indicate that the BFGSA can effectively be used for data clustering.
We present a framework that we are currently developing, that allows one to extract knowledge from the knowledge discovery in database (KDD) dataset. Data mining is a very active and space growing research area. Knowledge discovery in databases (KDD) is very useful in scientific domains. In simple terms, association rule mining is one of the most well-known methods for such knowledge discovery. Initially, database are divided into training and testing for the aid of fuzzy generating the rules using fuzzy rules generation the set of rules are generated from the given dataset. From the generated rules, we are extracting the significant rules by using the improved artificial bee colony algorithm and cuckoo search algorithm (IABCCS). After extracting optimal knowledge from the dataset via rules, the data will be classified using fuzzy classifier with the aid of this finally we will classify the attack and normal.
Subgroup Discovery is a broadly applicable supervised local pattern mining method to search relations between different properties with respect to a target variable. With the exponential growth in data storage, the massive data gathered has hampered the performance of current techniques. In this regard, our aim is to propose two new algorithms to discover subgroups on Big Data by using MapReduce. Apache Spark was used to tackle the Big Data requirements. The experimental study includes more than 50 large datasets and a set of efficient algorithms. Search spaces bigger than 1.276 · 1015 subgroups are used. The experimental study reveals the alluring results in efficiency when optimistic estimates are considered, as well as demonstrating the usefulness of using Apache Spark to tackle Big Data.
The proposed new hybrid approach for data clustering is achieved by initially exploiting spatial fuzzy c-means for clustering the vertex into homogeneous regions. Further to improve the fuzzy c-means with its achievement in segmentation, we make use of gravitational search algorithm which is inspired by Newton’s rule of gravity. In this paper, a modified modularity measure to optimize the cluster is presented. The technique is evaluated under standard metrics of accuracy, sensitivity, specificity, Map, RMSE and MAD. From the results, we can infer that the proposed technique has obtained good results.
Gravitational search algorithm (GSA) has shown an effective performance for solving real-world optimization problems. However, it suffers from premature convergence because of quick losing of diversity. To enhance its performance, this paper proposes a novel GSA algorithm, called GSA–PWL (piecewise linear)–SQP (sequential quadratic programming), which employs a diversity enhancing mechanism and an accelerated local search strategy to achieve a trade-off between exploration and exploitation abilities. A comprehensive experimental study is conducted on a set of benchmark functions. Comparison results show that GSA–PWL–SQP obtains a promising performance on the majority of the test problems. Furthermore, the GSA–PWL–SQP is applied to data fitting with B-splines to solve very difficult continuous multimodal and multivariate nonlinear optimization problem. The method of data fitting based on GSA–PWL–SQP yields very accurate results even for curves with singularities and/or cusps and is very efficient in terms of data points error, AIC and BIC criteria.
A hybrid model of Genetic Algorithm (GA) with local search to discover linguistic summaries and its application into the creep data analysis is proposed in this paper. Two specifics operator and a called Diversity term in the fitness function are introduced by the model to guarantees summaries with high quality and a wide range of information respectively. The experiments show that the hybrid model improves the results compared to those obtained using the classical model of GA. The quality of the summaries was verified by the interpretation of some of them from the theoretical point of view.
Clustering is a very important technique in knowledge discovery. It has been widely used in data mining, image processing, machine learning, bioinformatics, marketing and other fields. Clustering discern the objects into groups called clusters, based on certain criteria. The similarity of objects is high within the clusters, but low between the clusters. In this work, we investigate a hybridization of the gravitational search algorithm GSA and big bang-big crunch algorithm BB-BC on data clustering. In the proposed approach, namely GSA-BB, GSA is used to explore the search space for finding the optimal locations of the clusters centroids. Whenever GSA loses its exploration, BB-BC algorithm is used to diversify the population. The performance of the proposed method is compared with GSA, BB-BC and K-means algorithms using six standard and real datasets taken from the UCI machine learning repository. Experimental results indicate that there is significant improvement in the quality of the clusters obtained by the proposed hybrid method over the non-hybrid methods.
Abstract. Software testing is a very important phase in the development of software. Testing includes the generation of test cases which, if done manually, is time consuming. To automate this process and generate optimal test cases, several meta-heuristic techniques have been developed. These approaches include genetic algorithm, cuckoo search, tabu search, intelligent water drop, etc. This paper presents an effective approach for test data generation using the cuckoo search and tabu search algorithms (CSTS). It combines the cuckoo algorithm's strength of converging to the solution in minimal time along with the tabu mechanism of backtracking from local optima by Lévy flight. The experimental results show that the algorithm is effective in generating test cases optimally and its performance is better than various earlier proposed approaches.
Data clustering is an important technique in data mining. It is a method of partitioning data into clusters, in which each cluster must have data of great similarity and different clusters must have data of high dissimilarity. A lot of clustering algorithms are found in the literature. In general, there is no single algorithm that is suitable for all types of data, conditions and applications. Each algorithm has its own advantages, limitations and shortcomings. Therefore, introducing novel and effective approaches for data clustering is an open and active research area. This paper presents a novel binary search algorithm for data clustering that not only finds high quality clusters but also converges to the same solution in different runs. In the proposed algorithm a set of initial centroids are chosen from different parts of the test dataset and then optimal locations for the centroids are found by thoroughly exploring around of the initial centroids. The simulation results using six benchmark datasets from the UCI Machine Learning Repository indicate that proposed algorithm can efficiently be used for data clustering.
In this chapter, we survey methods that perform keyword search on graph data. Keyword search provides a simple but user-friendly interface to retrieve information from complicated data structures. Since many real life datasets are represented by trees and graphs, keyword search has become an attractive mechanism for data of a variety of types. In this survey, we discuss methods of keyword search on schema graphs, which are abstract representation for XML data and relational data, and methods of keyword search on schema-free graphs. In our discussion, we focus on three major challenges of keyword search on graphs. First, what is the semantics of keyword search on graphs, or, what qualifies as an answer to a keyword search; second, what constitutes a good answer, or, how to rank the answers; third, how to perform keyword search efficiently. We also discuss some unresolved challenges and propose some new research directions on this topic.
BackgroundLiquid chromatography coupled with tandem mass spectrometry (LC-MS/MS) has become one of the most used tools in mass spectrometry based proteomics. Various algorithms have since been developed to automate the process for modern high-throughput LC-MS/MS experiments.ResultsA probability based statistical scoring model for assessing peptide and protein matches in tandem MS database search was derived. The statistical scores in the model represent the probability that a peptide match is a random occurrence based on the number or the total abundance of matched product ions in the experimental spectrum. The model also calculates probability based scores to assess protein matches. Thus the protein scores in the model reflect the significance of protein matches and can be used to differentiate true from random protein matches.ConclusionThe model is sensitive to high mass accuracy and implicitly takes mass accuracy into account during scoring. High mass accuracy will not only reduce false positives, but also improves the scores of true positive matches. The algorithm is incorporated in an automated database search program MassMatrix.
Most of the content-based image retrieval systems require a distance computation for each candidate image in the database. As a brute-force approach, the exhaustive search can be employed for this computation. However, this exhaustive search is time-consuming and limits the usefulness of such systems. Thus, there is a growing demand for a fast algorithm which provides the same retrieval results as the exhaustive search. In this paper, we prose a fast search algorithm based on a multi-resolution data structure. The proposed algorithm computes the lower bound of distance at each level and compares it with the latest minimum distance, starting from the low-resolution level. Once it is larger than the latest minimum distance, we can exclude the candidates without calculating the full- resolution distance. By doing this, we can dramatically reduce the total computational complexity. It is noticeable that the proposed fast algorithm provides not only the same retrieval results as the exhaustive search, but also a faster searching ability than existing fast algorithms. For additional performance improvement, we can easily combine the proposed algorithm with existing tree-based algorithms. The algorithm can also be used for the fast matching of various features such as luminance histograms, edge images, and local binary partition textures.
Abstract In real life, data often appear in the form of sequences and this form of data is called sequence data. In this paper, a new definition on sequence similarity and a novel algorithm, Projection Algorithm, for sequence data searching are proposed. This algorithm is not required to access every datum in a sequence database. However, it guarantees that no qualified subsequence is falsely rejected. Moreover, the projection algorithm can be extended to match subsequences with different scales. With careful selection of parameters, most of the similar subsequences with different scales can be retrieved. We also show by experiments that the proposed algorithm can outperform the traditional sequential searching algorithm up to 96 times in terms of speed up.
This paper describes a data-interlacing architecture with two-dimensional (2-D) data-reuse for full-search blockmatching algorithm. Based on a one-dimensional processing element (PE) array and two data-interlacing shift-register arrays, the proposed architecture can efficiently reuse data to decrease external memory accesses and save the pin counts. It also achieves 100% hardware utilization and a high throughput rate. In addition, the same chips can be cascaded for different block sizes, search ranges, and pixel rates.
We introduce a new multidimensional pattern matching problem that is a natural generalization of string matching, a well studied problem1. The motivation for its algorithmic study is mainly theoretical. LetA1:n1,?,1:nd be a text matrix withN=n1?ndentries andB1:m1,?,1:mr be a pattern matrix withM=m1?mrentries, whered?r?1 (the matrix entries are taken from an ordered alphabet ?). We study the problem of checking whether somer-dimensional submatrix ofAis equal toB(i.e., adecisionquery).Acan be preprocessed andBis given on-line. We define a new data structure for preprocessingAand propose CRCW-PRAM algorithms that build it inO(logN) time withN2/nmaxprocessors, wherenmax=max(n1,?,nd), such that the decision query forBtakesO(M) work andO(logM) time. By using known techniques, we would get the same preprocessing bounds but anO((dr)M) work bound for the decision query. The latter bound is undesirable since it can depend exponentially ond; our bound, in contrast, is independent ofdand optimal. We can also answer, in optimal work, two further types of queries: (a) anenumerationquery retrieving all ther-dimensional submatrices ofAthat are equal toBand (b) anoccurrencequery retrieving only the distinct positions inAthat correspond to all of these submatrices. As a byproduct, we also derive the first efficient sequential algorithms for the new problem.
We present here a codification structure, entirely interfaced with the main packages for biomolecule database management, associated with a new search algorithm to retrieve quickly a sequence in a database. This system is derived from a method previously proposed for homology search in databanks with a preprocessed codification of an entire database in which all the overlapping subsequences of a specific length in a sequence were converted into a code and stored in a hash-coding file. This new algorithm is designed for an improved use of the codification. It is based on the recognition of the rarest strings which characterize the query sequence and the intersection of sorted lists read in the codification structure. The system is applicable to both nucleic acid and protein sequences and is used to find patterns in databanks or large sets of sequences. A few examples of applications are given. In addition, the comparison of our method with existing ones shows that this new approach speeds up the search for query patterns in large data sets.
In order to ensure the security of logistics information and to query information quickly and efficiently, using searchable encryption algorithms, combined with the characteristics of the blockchain, a searchable and encrypted logistics information blockchain data query algorithm is proposed. First, the logistics information is divided into multiple data files, encrypted with an asymmetric searchable encryption algorithm, and then stored in the cloud server. The keyword index value is extracted from each data file and uploaded to the blockchain. This solution can be used at any time Update and query data. Finally, analyze the correctness, completeness and safety of the scheme of this article, which proves the feasibility of this scheme.
Abstract Cluster analysis is a valuable data analysis and data mining technique. Nature-inspired population-based metaheuristics are promising search methods for solving optimization problems including data clustering. In this paper, a recently proposed algorithm called the water cycle algorithm, based on the evaporation rate is used in conjunction with a local search method namely Hookes and Jeeves method to perform data clustering. Statistical analyses were carried out which show that the hybrid optimization method, in general, performs superior to the methods reported in the literature in terms of solution quality as well as computational performance. The proposed hybrid algorithm is tested on some selected standard datasets obtained from the UCI machine-learning repository. The objective function is based on the Euclidean distance as well as the DB index. The experimental results were compared with the data clustering results reported in published literature. The simulation results confirm the superiority of the proposed hybrid method as an efficient and reliable algorithm to solve clustering problems.
Abstract Biclustering is an unsupervised classification technique that plays an increasingly important role in the study of modern biology. This data mining technique has provided answers to several challenges raised by the analysis of biological data and more particularly the analysis of gene expression data. It aims to cluster simultaneously genes and conditions. These unsupervised techniques are based essentially on the assumption that the extraction of the co-expressed genes allows to have co-regulated genes. In addition, the integration of biological information in the search process may induce to the extraction of relevant and non-trivial biclusters. Therefore, this work proposes an evolutionary algorithm based on local search method that relies on biological knowledge. An experimental study is achieved on real microarray datasets to evaluate the performance of the proposed algorithm. The assessment and the comparison are based on statistical and biological criteria. A cross-validation experiment is also used to estimate its accuracy. Promising results are obtained. They demonstrate the importance of the integration of the biological knowledge in the biclustering process to foster the efficiency and to promote the discovery of non-trivial and biologically relevant biclusters.
Abstract Computing the periods of variable objects is well-known to be computationally expensive. Modern astronomical catalogs contain a significant number of observed objects. Therefore, even if the period ranges for particular classes of objects are well-constrained due to expected physical properties, periods must be derived for a tremendous number of objects. In this paper, we propose a GPU-accelerated Lomb–Scargle period finding algorithm that computes periods for single objects or for batches of objects as is necessary in many data processing pipelines. We demonstrate the performance of several optimizations, including comparing the use of shared and global memory GPU kernels and using multiple CUDA streams to copy periodogram data from the GPU to the host. Also, we quantify the difference between 32-bit and 64-bit floating point precision on two classes of GPUs, and show that the performance degradation of using 64-bit over 32-bit is greater on the CPU than a GPU designed for scientific computing. We find that the GPU algorithm achieves superior performance over the baseline parallel CPU implementation, achieving a speedup of up to 174.53 × . The Vera C. Rubin Observatory will carry out the Legacy Survey of Space and Time (LSST). We perform an analysis that shows we can derive the rotation periods of batches of Solar System objects at LSST scale in near real-time, which will be employed in a future LSST event broker. All source code has been made publicly available.
Abstract Motivation The Flavivirus genus includes several important pathogens, such as Zika, dengue and yellow fever virus. Flavivirus RNA genomes contain a number of functionally important structures in their 3′ untranslated regions (3′UTRs). Due to the diversity of sequences and topologies of these structures, their identification is often difficult. In contrast, predictions of such structures are important for understanding of flavivirus replication cycles and development of antiviral strategies. Results We have developed an algorithm for structured pattern search in RNA sequences, including secondary structures, pseudoknots and triple base interactions. Using the data on known conserved flavivirus 3′UTR structures, we constructed structural descriptors which covered the diversity of patterns in these motifs. The descriptors and the search algorithm were used for the construction of a database of flavivirus 3′UTR structures. Validating this approach, we identified a number of domains matching a general pattern of exoribonuclease Xrn1-resistant RNAs in the growing group of insect-specific flaviviruses. Availability and implementation The Leiden Flavivirus RNA Structure Database is available at https://rna.liacs.nl. The search algorithm is available at https://github.com/LeidenRNA/SRHS. Supplementary information Supplementary data are available at Bioinformatics online.
The rapid growth in biomedical datasets has generated high dimensionality features that negatively impact machine learning classifiers. In machine learning, feature selection (FS) is an essential process for selecting the most significant features and reducing redundant and irrelevant features. In this study, an equilibrium optimization algorithm (EOA) is used to minimize the selected features from high-dimensional medical datasets. EOA is a novel metaheuristic physics-based algorithm and newly proposed to deal with unimodal, multi-modal, and engineering problems. EOA is considered as one of the most powerful, fast, and best performing population-based optimization algorithms. However, EOA suffers from local optima and population diversity when dealing with high dimensionality features, such as in biomedical datasets. In order to overcome these limitations and adapt EOA to solve feature selection problems, a novel metaheuristic optimizer, the so-called improved equilibrium optimization algorithm (IEOA), is proposed. Two main improvements are included in the IEOA: The first improvement is applying elite opposite-based learning (EOBL) to improve population diversity. The second improvement is integrating three novel local search strategies to prevent it from becoming stuck in local optima. The local search strategies applied to enhance local search capabilities depend on three approaches: mutation search, mutation–neighborhood search, and a backup strategy. The IEOA has enhanced the population diversity, classification accuracy, and selected features, and increased the convergence speed rate. To evaluate the performance of IEOA, we conducted experiments on 21 biomedical benchmark datasets gathered from the UCI repository. Four standard metrics were used to test and evaluate IEOA’s performance: the number of selected features, classification accuracy, fitness value, and p-value statistical test. Moreover, the proposed IEOA was compared with the original EOA and other well-known optimization algorithms. Based on the experimental results, IEOA confirmed its better performance in comparison to the original EOA and the other optimization algorithms, for the majority of the used datasets.
This paper proposes a hybrid approach for solving data clustering problems. This hybrid approach used one of the swarm intelligence algorithms (SIAs): grasshopper optimization algorithm (GOA) due to its robustness and effectiveness in solving optimization problems. In addition, a local search (LS) strategy is applied to enhance the solution quality and access to optimal data clustering. The proposed algorithm is divided into two stages, the first of which aims to use GOA to prevent getting trapped in local minima and to find an approximate solution. While the second stage aims by LS to increase LS performance and obtain the best optimal solution. In other words, the proposed algorithm combines the exploitation capability of GOA and the discovery capability of LS, and integrates the merits of both GOA and LS. In addition, 7 well-known datasets that commonly used in several studies are used to validate the proposed technique. The results of the proposed methodology are compared to previous studies; where statistical analysis, for the various algorithms, indicated the superiority of the proposed methodology over other algorithms and its ability to solve this type of problem.
Feature selection has gained its importance due to the voluminous nature of the data. Owing to the computational complexity of wrapper approaches, the poor performance of filtering techniques, and the classifier dependency of embedded approaches, hybrid approaches are more commonly used in feature selection. Hybrid approaches use filtering metrics to reduce the computational complexity of wrapper algorithms and are proved to yield better feature subset. Though filtering metrics select the features based on their significance, most of them are unstable and biased towards the metric used. Moreover, the choice of filtering metrics depends largely on the distribution of data and data types. Biomedical datasets contain features with different distribution and types adding to the complexity in the choice of filtering metric. We address this problem by proposing a stable filtering method based on rank aggregation in hybrid feature selection model with Improved Squirrel search algorithm for biomedical datasets. Our proposed model is compared with other well-known and state-of-the-art methods and the results prove that our model exhibited superior performance in terms of classification accuracy and computational time. The robustness of our proposed model is proved by conducting experiments on nine biomedical datasets and with three different classifiers.
PurposeDifferential search algorithm (DSA) is a new optimization, meta-heuristic algorithm. It simulates the Brownian-like, random-walk movement of an organism by migrating to a better position. The purpose of this paper is to analyze the performance analysis of DSA into two key parts: six random number generators (RNGs) and Benchmark functions (BMF) from IEEE World Congress on Evolutionary Computation (CEC, 2015). Noting that this study took problem dimensionality and maximum function evaluation (MFE) into account, various configurations were executed to check the parameters’ influence. Shifted rotated Rastrigin’s functions provided the best outcomes for the majority of RNGs, and minimum dimensionality offered the best average. Among almost all BMFs studied, Weibull and Beta RNGs concluded with the best and worst averages, respectively. In sum, 50,000 MFE provided the best results with almost RNGs and BMFs.Design/methodology/approachDSA was tested under six randomizers (Bernoulli, Beta, Binomial, Chisquare, Rayleigh, Weibull), two unimodal functions (rotated high conditioned elliptic function, rotated cigar function), three simple multi-modal functions (shifted rotated Ackley’s, shifted rotated Rastrigin’s, shifted rotated Schwefel’s functions) and three hybrid Functions (Hybrid Function 1 (n=3), Hybrid Function 2 (n=4,and Hybrid Function 3 (n=5)) at four problem dimensionalities (10D, 30D, 50D and 100D). According to the protocol of the CEC (2015) testbed, the stopping criteria are the MFEs, which are set to 10,000, 50,000 and 100,000. All algorithms mentioned were implemented on PC running Windows 8.1, i5 CPU at 1.60 GHz, 2.29 GHz and a 64-bit operating system.FindingsThe authors concluded the results based on RNGs as follows: F3 gave the best average results with Bernoulli, whereas F4 resulted in the best outcomes with all other RNGs; minimum and maximum dimensionality offered the best and worst averages, respectively; and Bernoulli and Binomial RNGs retained the best and worst averages, respectively, when all other parameters were fixed. In addition, the authors’ results concluded, based on BMFs: Weibull and Beta RNGs produced the best and worst averages with most BMFs; shifted and rotated Rastrigin’s function and Hybrid Function 2 gave rise to the best and worst averages. In both parts, 50,000 MFEs offered the best average results with most RNGs and BMFs.Originality/valueBeing aware of the advantages and drawbacks of DS enlarges knowledge about the class in which differential evolution belongs. Application of that knowledge, to specific problems, ensures that the possible improvements are not randomly applied. Strengths and weaknesses influenced by the characteristics of the problem being solved (e.g. linearity, dimensionality) and by the internal approaches being used (e.g. stop criteria, parameter control settings, initialization procedure) are not studied in detail. In-depth study of performance under various conditions is a “must” if one desires to efficiently apply DS algorithms to help solve specific problems. In this work, all the functions were chosen from the 2015 IEEE World Congress on Evolutionary Computation (CEC, 2015).
PurposeThe issues of radiating sources in the existence of smooth convex matters by such objects are of huge significance in the modeling of antennas on structures. Conformal antenna arrays are necessary when an antenna has to match to certain platforms. A fundamental problem in the design is that the possible surfaces for a conformal antenna are infinite in number. Furthermore, if there is no symmetry, each element will see a different environment, and this complicates the mathematics. As a consequence, the element factor cannot be factored out from the array factor.Design/methodology/approachThis paper intends to enhance the design of the conformal antenna. Here, the main objective of this task is to maximize the antenna gain and directivity from the first-side lobe and other side-lobes in the two way radiation pattern. Thus the adopted model is designed as a multiobjective concern. In order to attain this multiobjective function, both the element spacing and the radius of each antenna element should be optimized based on the probability of the Crow Search Algorithm (CSA). Thus the proposed method is named Probability Improved CSA (PI-CSA). Here, the First Null Beam Width (FNBW) and Side-Lobe Level (SLL) are minimized. Moreover, the adopted scheme is compared with conventional algorithms, and the results are attained.FindingsFrom the analysis, the gain of the presented PI-CSA scheme in terms of best performance was 52.68% superior to ABC, 25.11% superior to PSO, 13.38% superior to FF and 3.21% superior to CS algorithms. Moreover, the mean performance of the adopted model was 62.94% better than ABC, 13.06% better than PSO, 24.34% better than FF and 10.05% better than CS algorithms. By maximizing the gain and directivity, FNBW and SLL were decreased. Thus, the optimal design of the conformal antenna has been attained by the proposed PI-CSA algorithm in an effective way.Originality/valueThis paper presents a technique for enhancing the design of the conformal antenna using the PI-CSA algorithm. This is the first work that utilizes PI-CSA-based optimization for improving the design of the conformal antenna.
The advent of virtualization technology has created a huge potential application for cloud computing. In virtualization, a large hardware resource is often broken down into smaller virtual units. These small units are then provisioned to different clients. However, these services need to be provided in such a way that resources are properly utilized. To achieve this, many of the scheduling, allocation, and provisioning issues of data centers are formulated as optimization problems. The virtual machine placement problem (VMPP) is a typical provisioning problem of data centers. In VMPP, several virtual machine requests are to be hosted on physical machines such that a minimum number of physical machines are used. This work proposes a cuckoo search (CS) inspired algorithm for solving the VMPP. To improve the algorithm’s performance, new cost and perturbation functions are developed. The proposed method was tested on two well-known benchmark datasets. It outperformed the reordered grouping genetic algorithm, best-fit decreasing, first-fit decreasing, and an earlier CS method called multiCSA.
Approximate Nearest Neighbor Search (ANNS) is a fundamental problem in many areas of machine learning and data mining. During the past decade, numerous hashing algorithms are proposed to solve this problem. Every proposed algorithm claims to outperform Locality Sensitive Hashing (LSH), which is the most popular hashing method. However, the evaluation of these hashing article was not thorough enough, and the claim should be re-examined. If implemented correctly, almost all the hashing methods will have their performance improved as the code length increases. However, many existing hashing article only report the performance with the code length shorter than 128. In this article, we carefully revisit the problem of search-with-a-hash-index and analyze the pros and cons of two popular hash index search procedures. Then we proposed a simple but effective novel hash index search approach and made a thorough comparison of eleven popular hashing algorithms. Surprisingly, the random-projection-based Locality Sensitive Hashing ranked the first, which is in contradiction to the claims in all the other 10 hashing article. Despite the extreme simplicity of random-projection-based LSH, our results show that the capability of this algorithm has been far underestimated. For the sake of reproducibility, all the codes used in the article are released on GitHub, which can be used as a testing platform for a fair comparison between various hashing algorithms.
Given a large graph representing relations between entities, searching for complex relationships (called semantic associations, or SAs for short) between a set of entities is a common type of information needs in many domains. Further, numerous SAs are often abstracted into a few frequent high-level conceptual graph patterns (called SA patterns, or SAPs for short), which organize SAs into interpretable subgroups. Whereas the quality and usefulness of SAs and SAPs have been extensively studied in the literature, in this article we aim to develop faster algorithms for SA search and frequent SAP mining. For the former problem, we leverage distances to prune the search space, and implement a distance oracle to balance the time and space for distance calculation. For the latter problem, we exploit both graph structure and labels to induce fine-grained skeleton-based partitions of SAs, which may be pruned to reduce SAP enumeration. Besides, we generate canonical codes for SAs, which not only enable result deduplication but also are reused in SAP mining to improve the overall performance. We extensively evaluate the efficiency of our algorithms on four large graphs, using both random queries and simulated queries which reproduce the extreme case of finding numerous SAs.
Mining cohesive subgraphs from a network is a fundamental problem in network analysis. Most existing cohesive subgraph models are mainly tailored to unsigned networks. In this paper, we study the problem of seeking cohesive subgraphs in a signed network, in which each edge can be positive or negative, denoting friendship or conflict, respectively. We propose a novel model, called maximal <inline-formula><tex-math notation="LaTeX">$(\alpha, k)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wang-ieq1-2904569.gif"/></alternatives></inline-formula>-clique, that represents a cohesive subgraph in signed networks. Specifically, a maximal <inline-formula><tex-math notation="LaTeX">$(\alpha, k)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wang-ieq2-2904569.gif"/></alternatives></inline-formula>-clique is a clique in which every node has at most <inline-formula><tex-math notation="LaTeX">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href="wang-ieq3-2904569.gif"/></alternatives></inline-formula> negative neighbors and at least <inline-formula><tex-math notation="LaTeX">$\lceil \alpha k \rceil$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>⌈</mml:mo><mml:mi>α</mml:mi><mml:mi>k</mml:mi><mml:mo>⌉</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wang-ieq4-2904569.gif"/></alternatives></inline-formula> positive neighbors (<inline-formula><tex-math notation="LaTeX">$\alpha \geq 1$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>α</mml:mi><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="wang-ieq5-2904569.gif"/></alternatives></inline-formula>). We show that the problem of enumerating all maximal <inline-formula><tex-math notation="LaTeX">$(\alpha, k)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wang-ieq6-2904569.gif"/></alternatives></inline-formula>-cliques in a signed network is NP-hard. To enumerate all maximal <inline-formula><tex-math notation="LaTeX">$(\alpha, k)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wang-ieq7-2904569.gif"/></alternatives></inline-formula>-cliques efficiently, we first develop an elegant signed network reduction technique to significantly prune the signed network. Then, we present an efficient branch and bound enumeration algorithm with several carefully-designed pruning rules to enumerate all maximal <inline-formula><tex-math notation="LaTeX">$(\alpha, k)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wang-ieq8-2904569.gif"/></alternatives></inline-formula>-cliques in the reduced signed network. In addition, we also propose an efficient algorithm with three novel upper-bounding techniques to find the maximum <inline-formula><tex-math notation="LaTeX">$(\alpha, k)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wang-ieq9-2904569.gif"/></alternatives></inline-formula>-clique in a signed network. The results of extensive experiments on five large real-life datasets demonstrate the efficiency, scalability, and effectiveness of our algorithms.
The find of density peak clustering algorithm (FDP) has poor performance on high-dimensional data. This problem occurs because the clustering algorithm ignores the feature selection. All features are evaluated and calculated under the same weight, without distinguishing. This will lead to the final clustering effect which cannot achieve the expected. Aiming at this problem, we propose a new method to solve it. We calculate the importance value of all features of high-dimensional data and calculate the mean value by constructing random forest. The features whose importance value is less than 10% of the mean value are removed. At this time, we extract the important features to form a new dataset. At this time, improved t-SNE is used for dimension reduction, and better performance will be obtained. This method uses t-SNE that is improved by the idea of random forest to reduce the dimension of the original data and combines with improved FDP to compose the new clustering method. Through experiments, we find that the evaluation index NMI of the improved algorithm proposed in this paper is 23% higher than that of the original FDP algorithm, and 9.1% higher than that of other clustering algorithms (
 
 K
 
 -means, DBSCAN, and spectral clustering). It has good performance in high-dimensional datasets that are verified by experiments on UCI datasets and wireless sensor networks.
The cloud computing is interlinked with recent and out-dated technology. The cloud data storage industry is earning billion and millions of money through this technology. The cloud remote server storage is on-demand technology. The cloud users are expecting higher quality in minimal cost. The quality of service is playing a vital role in any latest technology. The cloud user always depends on thirty party service providers. This service provider is facing higher competition. The customer is choosing a service based on two parameters one is security and another one is cost. The reason behind this is all our personal data is stored on some third party server. The customer is expecting higher security level. The service provider is choosing many techniques for data security, best one is encryption mechanism. This encryption method is having many algorithms. Then again one problem is raised, that is which algorithm is best for encryption. The prediction of algorithm is one of major task. Each and every algorithm is having unique advantage. The algorithm performance is varying depends on file type. The proposed method of this article is to solve this encryption algorithm selection problem by using tabu search concept. The proposed method is to ensure best encryption method to reducing the average encode and decode time in multimedia data. The local search scheduling concept is to schedule the encryption algorithm and store that data in local memory table. The quality of service is improved by using proposed scheduling technique.
The 3-dimensional bin packing problem (3D-BPP) is not only fundamental in combinatorial optimization but also widely applied in real world logistics. In the modern logistics industry, the complexity of constraints, heterogeneity of cargoes and scale of orders are dramatically increased, leading to great challenges to devise packing plans up to standard. While the tree search algorithm is proved to be a successful paradigm to solve the 3D-BPP, it is too time-consuming to be applied in the aforementioned large-scale scenarios. To overcome the limitation, we propose a data-driven tree search algorithm (DDTS) to tackle the 3D-BPP. The solution space with complicated constraints is explored by a tree search algorithm, and a convolutional neural network trained with historical data guides pruning the tree so as to accelerate the search process. Computational experiments on real-world datasets show that our algorithm outperforms the state-of-the-art approach with a loading rate improvement of 2.47%. Moreover, the deep learning technique increases searching efficiency by 37.14% with only 0.04% performance loss. The algorithm has been deployed in Huawei Logistics System, which increases the loading rate by 3% and could reduce the logistics cost by millions of dollars per year. To the best of our knowledge, we are the first to embed pruning networks into tree search for the large-scale 3D-BPP.
In the past few decades, the field of bioinformatics has accumulated a large amount of gene expression data which provided important support for the diagnosis of disease. However, high dimensionality, small sample sizes, and redundant features often adversely affect the accuracy and the speed of prediction. Existing feature selection models cannot obtain the information of these datasets accurately. Filter and wrapper are two commonly used feature selection methods. Combining the advantages of the fast calculation speed of the filter and the high accuracy of the wrapper, a new hybrid algorithm called MIIBGSA, is proposed, which hybridizes mutual information and improved Gravitational Search Algorithm (GSA). First, mutual information is used to rank and select important features, these features are further chosen into the population of the wrapper method. Then, due to the effectiveness of the GSA algorithm, GSA is adopted to further seek an optimal feature subset. However, GSA also has the disadvantages of slow search speed and premature convergence, which limit its optimization ability. In our work, a scale function is added to the speed update to enhance its searchability, and an adaptive ${k}_{best}$ particle update formula is proposed to improve its convergence accuracy and propose a fitness sharing strategy to enhance the randomness of particle populations and searchability through the niche algorithm of fitness sharing. We used 10-fold-CV method with the KNN classifier to evaluate the classification accuracy. Experimental results on five publicly available high-dimensional biomedical data sets show that the proposed MI-IBGSA has superior performance than other algorithms.
The aim of the study is to review and apply machine learning methods and algorithms to detect abnormal user behavior based on text analysis. This article provides an overview of the methods, algorithms and approaches used in the software application under development. Application of machine learning methods and algorithms in implementation of the software application is proposed. Machine learning methods and algorithms used in the UBA system can solve the problems of analyzing data of various directions. Data abnormalities finding ensures a timely response to deviations from the user's behavioral profile, which allows to maintain the integrity of the target data.
Microarray data play a huge role in recognizing a proper cancer diagnosis and classification. In most microarray data set consist of thousands of genes, but the majority number of genes are irrelevant to the diseases. An efficient algorithm for gene selection becomes important to deal with large microarray data. The main challenge is to analyze and select the relevant genes with maximum classification accuracy. Various algorithms were proposed for gene classification in previous studies, however, limited success was succeeded due to the selection of many genes in the high-dimensional microarray data. This study proposed and developed a hybrid multi-objective cuckoo search with evolutionary operators for gene selection. Evolutionary operators that are used in this article were double mutation and single crossover operators. The motivation behind this research is to improve the dimensions’ values and explorative search abilities. Multi-objective cuckoo search with evolutionary operators employed the selection of informative genes among the high-dimensional cancer microarray data. Experiments were conducted on seven publicly available and high-dimensional cancer microarray data sets. These microarray data sets consist of approximately 2000 to 15000 genes. The results from the experiments concluded that the developed algorithm, multi-objective cuckoo search with evolutionary operators outperforms cuckoo search and multi-objective cuckoo search algorithms with a smaller number of selected significant genes.
Clustering as an unsupervised learning method is a process of dividing a data object or observation object into a subset, that is to classify the data through observation learning instead of example learning without the guidance of the prior class label information. Bat algorithm (BA) is a swarm intelligence optimization algorithm inspired by bat’s ultrasonic echo localization foraging behavior, but it has the disadvantages of being easily trapped into local minima and not being highly accurate. So an improved bat algorithm was proposed. In the global search, a Gaussian-like convergence factor is added, and five different convergence factors are proposed to improve the global optimization ability of the algorithm. In the local search, the hunting mechanism of the whale optimization algorithm (WOA) and the sine position updating strategy are adopted to improve the local optimization ability of the algorithm. This paper compares the clustering effect of the improved bat algorithm with bat algorithm, flower pollination algorithm (FPA), harmony search (HS) algorithm, whale optimization algorithm and particle swarm optimization (PSO) algorithm on seven real data sets under six different convergence factors. The simulation results show that the clustering effect of the improved bat algorithm is superior to other intelligent optimization algorithms.
Abstract Big Data optimization (Big-Opt) refers to optimization problems which require to manage the properties of big data analytics. In the present paper, the Search Manager (SM), a recently proposed framework for hybridizing metaheuristics to improve the performance of optimization algorithms, is extended for multi-objective problems (MOSM), and then five configurations of it by combination of different search strategies are proposed to solve the EEG signal analysis problem which is a member of the big data optimization problems class. Experimental results demonstrate that the proposed configurations of MOSM are efficient in this kind of problems. The configurations are also compared with NSGA-III with uniform crossover and adaptive mutation operators (NSGA-III UCAM), which is a recently proposed method for Big-Opt problems.
Abstract This paper presents a variant of harmony search algorithm (HS), called best–worst-mean harmony search (BWM_HS). The main difference between the proposed algorithm and the canonical HS is that it employs a modified memory consideration procedure to utilize more efficiently the accumulated knowledge and experience in harmony memory (HM). To this aim, the random harmony selection scheme of this procedure is replaced with three novel pitch selection and production rules. These rules use the information of the current best and worst harmonies as well as the mean of all harmonies to guide the search process. To further utilize the valuable information of HM, two new harmonies are generated at each iteration where the better one will compete with the current worst harmony. The mean of all harmonies is always employed to produce a new harmony. On the other hand, each pitch of the second one is obtained by the rules that consider the information of the best and worst harmonies. These rules can present either explorative or exploitative search behaviors at different stages of search. Thus, a probabilistic self-adaptive selection scheme decides to choose between them to properly balance the exploration and exploitation abilities. The general performance of BWM_HS for solving optimization problems is evaluated against CEC 2017 benchmark functions and its results are compared with HS and eight state-of-the-art variants of HS. The comparison indicates that the performance of BWM_HS is better than or equal to the compared algorithms with respect to the accuracy, robustness, and convergence speed criteria. Moreover, the performance of BWM_HS in solving clustering problems is investigated by applying it for clustering several well-known benchmark datasets. The experimental results show that, in general, the BWM_HS outperforms other well-known algorithms in the literature and in particular, it significantly improves the statistical results for one dataset.
The complexity and high dimensionality are the inherent concerns of big data. The role of feature selection has gained prime importance to cope with the issue by reducing dimensionality of datasets. The compromise between the maximum classification accuracy and the minimum dimensions is as yet an unsolved puzzle. Recently, Monte Carlo Tree Search (MCTS)-based techniques have been invented that have attained great success in feature selection by constructing a binary feature selection tree and efficiently focusing on the most valuable features in the features space. However, one challenging problem associated with such approaches is a tradeoff between the tree search and the number of simulations. In a limited number of simulations, the tree might not meet the sufficient depth, thus inducing biasness towards randomness in feature subset selection. In this paper, a new algorithm for feature selection is proposed where multiple feature selection trees are built iteratively in a recursive fashion. The state space of every successor feature selection tree is less than its predecessor, thus increasing the impact of tree search in selecting best features, keeping the MCTS simulations fixed. In this study, experiments are performed on 16 benchmark datasets for validation purposes. We also compare the performance with state-of-the-art methods in literature both in terms of classification accuracy and the feature selection ratio.
Abstract Over the last decade, the number of studies on machine learning has significantly increased. One of the most widely researched areas of machine learning is data classification. Most big data systems require a large amount of information storage for analytic purposes; however, this involves some disadvantages, such as the costs of processing and collecting data. Thus, many researchers and practitioners are working on effectively reducing the number of features used in classification. This paper proposes a method which jointly optimizes both feature selection and classification. A survey of the relevant literature shows that the vast majority of studies focus on either feature selection or classification. In this study, the proposed parallel local search algorithm both selects features and finds a classifier with high rates of accuracy. Moreover, the proposed method is capable of finding solutions for problems that have extremely high numbers of features within a reasonable computation time.
It is important for Bayesian network (BN) structure learning, a NP-problem, to improve the accuracy and hybrid algorithms are a kind of effective structure learning algorithms at present. Most hybrid algorithms adopt the strategy of one heuristic search and can be divided into two groups: one heuristic search based on initial BN skeleton and one heuristic search based on initial solutions. The former often fails to guarantee globality of the optimal structure and the latter fails to get the optimal solution because of large search space. In this paper, an efficient hybrid algorithm is proposed with the strategy of two-stage searches. For first-stage search, it firstly determines the local search space based on Maximal Information Coefficient by introducing penalty factors p1, p2, then searches the local space by Binary Particle Swarm Optimization. For second-stage search, an efficient ADR (the abbreviation of Add, Delete, Reverse) algorithm based on three basic operators is designed to extend the local space to the whole space. Experiment results show that the proposed algorithm can obtain better performance of BN structure learning.
In this personalised web search (PWS), we utilise a kernel-based FCM for clustering a web pages. For effective personalised web search, queries are optimised using GSA with respect to clustered query sessions. In offline processing, initially preprocess the input information taken from consumer visited web pages and are transformed in to numerical matrix. These matrices are gathered with the help of kernel-based FCM method after produce a vector for consumer query and detect a minimum distance as centroid values these values are input to the GSA algorithm. It will engender these links given top N web pages from cluster. In online processing, the user query is engaged as input then extract some web pages from Google, Bing, Yahoo also extract content and snippet from web pages. Finally, detect a sum of contents and snippets and web pages would be considered in descending order.
Abstract Privacy-preserving data mining (PPDM) is a novel approach that has emerged in the market to take care of privacy issues. The intention of PPDM is to build up data-mining techniques without raising the risk of mishandling of the data exploited to generate those schemes. The conventional works include numerous techniques, most of which employ some form of transformation on the original data to guarantee privacy preservation. However, these schemes are quite multifaceted and memory intensive, thus leading to restricted exploitation of these methods. Hence, this paper intends to develop a novel PPDM technique, which involves two phases, namely, data sanitization and data restoration. Initially, the association rules are extracted from the database before proceeding with the two phases. In both the sanitization and restoration processes, key extraction plays a major role, which is selected optimally using Opposition Intensity-based Cuckoo Search Algorithm, which is the modified format of Cuckoo Search Algorithm. Here, four research issues, such as hiding failure rate, information preservation rate, and false rule generation, and degree of modification are minimized using the adopted sanitization and restoration processes.
The arrival of the era of big data has a great impact on the development of various industries in the society. There are abundant fine arts and cultural resources in the world, but its search is difficult and inefficient. Therefore, the rational development and utilization of artistic and cultural resources are to provide high-quality art and cultural products. At the same time, it is also an inevitable choice to accelerate the transformation of old and new. Power is to cultivate new forms of art and culture. In the context of big data, the cuckoo search algorithm is easy to implement due to its high efficiency. The parameters are rarely studied by various scholars and have been applied to solve optimization problems and search optimization problems. The application results show that it has relatively good performance. Big data searches for the context of artistic culture and artistic resources, whether in traditional painting, sculpture, technology, or the construction of knowledge and technology. Emerging design, photography, video, and the future of visual arts and phenomena, everyday life can build and convey personal attitudes, beliefs, and values of various visual images. Its search efficiency is not high and its accuracy is reduced. In order to solve the above problems, a cuckoo search algorithm (CFCS) based on change factors is proposed in the context of big data. Through data analysis and experiments with Matlab software, the results show that the overall convergence speed of the cuckoo search algorithm based on change factor is obviously better than that of the cuckoo search algorithm. Under the corresponding fitness conditions, the number of iterations of CFCS is significantly less than CS. The search efficiency of CFCS is higher than CS. The accuracy of CFCS is also significantly higher than CS.
Cloud computing delivers practical solutions for long-term image archiving systems. Cloud data centers consume enormous amounts of electrical energy that increases their operational costs. This shows the importance of investing on energy consumption techniques. Dynamic placement of virtual machines to appropriate physical nodes using metaheuristic algorithms is among the methods of reducing energy consumption. In metaheuristic algorithms, there should be a balance between both exploration and exploitation aspects so that they can find better solutions in a search space. Exploration means looking for a solution in a wider area, while exploitation is producing new solutions from existence ones. Artificial bee colony optimization, which is a biological metaheuristic algorithm, is a sign-oriented approach. It has a strong exploration ability, but a relatively weaker exploitation power. On the other hand, tabu search is a popular algorithm that shows better exploitation in comparison with ABC. In this study, cloud computing environments are detailed with an allocation protocol for efficient energy and resource management. The technique of energy-aware allocation splits data centers (DCs) resources among client applications end routes to enhance energy efficacy of DCs and also achieves anticipated quality of service (QoS) for everyone. Heuristic protocols are exercised for optimizing the distribution of resources to upgrade the efficiency of DC. In the current paper, energy-aware resources allotment technique is employed and optimized in clouds via a new approach called Tabu Job Master (JM). Tabu JM claims the benefits of some variables and also rapid convergence speeds. Results are duly achieved for energy consumption—the count of virtual machines (VMs) migration and also makespan. The results shown by Tabu JM are benchmarked by using genetic algorithm (GA), artificial bee colony (ABC), ABC with crossover and technique of mutation, the basic tabu search techniques, and Tabu Job Master.
Optical character recognition is becoming one of the widely researched areas in recent times. This research paper presents an optimization framework for ancient script recognition using the process of script or character segmentation. The proposed algorithm is based on evolutionary algorithm and capable of handing a continuous script of high-resolution data using concepts of big data. A hybrid combination of group search and firefly algorithm has been proposed in this research work and compared against recent works. Optimal classifications results are observed and recorded in this research paper.
This article presents a methodology for sizing transistors of a multistage, multipath capacitor-less feed-forward compensated operational amplifiers employed in advanced CMOS process implementation of continuous-time bandpass $\Sigma \Delta $ -modulators. This article describes the methodology: on system level, dealing with the placement of poles and zeros; and on the circuit level, discussing issues related to biasing, frequency response and other important performance metrics of the basic diff-pair. Algorithms are provided to simplify mathematical aspects of the work. The validity and the limitations of the proposed methodology are further discussed from the single-pole system used to model the individual amplification stages of the multistage amplifier. The worthiness of the proposed methodology in sizing the transistors of complex amplifier structures, such as the capacitor-less multistage, multipath feed-forward-compensated amplifiers is demonstrated using two design examples. A third-order amplifier with a dc gain of 52.6 dB and that reaches a unity-gain frequency of above 14 GHz while consuming only 5.6 mA from a 1-V supply; and a fourth-order amplifier with dc gain of 73.5 dB that achieves a 25.7-dB gain at 1 GHz while consuming 4.8 mW are designed in 28-nm CMOS FDSOI.
In the recent years, the researcher has to face new challenges due to the complexity of technological an advances in Wireless Sensor Network (WSN). Sensor based networks are a special category of a distributed network that is used to provide communication among the sensor nodes. The wireless sensor network consists of various classes of sensors like thermal, infrared, optical, and seismic for the measurement of temperature, heat, radiation, humidity which requires constant monitoring and detection in specific events. A sensor’s with limited functionality both in computations, battery voltage keeps periodic sensing physical or environmental of ecological factors. Sporadic events such as detecting border intrusion, flood detection and habitat exploration of animals. The design of a WSN depends drastically on continuity and coverage of the network. Connected with that energy constrained is considered as one of the important issues to balance the network load and to extend the network life. An optimal energy efficient cluster based routing algorithm is required for effective data diffusion. Conventional protocol like LEACH, HEED, PEGASIS protocol etc., fails to balance the network load and the coverage area when the sensor nodes are deployed in large scale. In harmony search algorithm (HSA) absence of gradient search leads the parameter search is in the local region where the required optimal solution remains outside the local region. HSA is heuristic algorithm uses random search with constant harmony memory consideration rate. This paper focuses on designing a Meta-heuristic optimized routing protocol for a distributed network using mutated harmony search algorithm (MHSA) to improve the energy efficiency by simultaneously analyzing the cluster patching. Cluster patching is examined for improving network coverage and connectivity thereby to optimize the energy distribution in WSN. MHSA is a refinement of heuristic algorithm in the global search by adjusting the harmony memory consideration rate HMCR. To improve the performance and efficiency exact balancing of diversification and intensification is done by varying the Pitch adjusting rate PAR and bandwidth BW. Parametric results are compared with the standard heuristics algorithm HSA, GA, and PSO. The computational time and the experimental results show the proposed MHSA gives 85% of connectivity improved that ensures the success of cluster formation for an increased number of nodes to increase increases the network lifetime when compared with existing algorithms.
We propose an improved analysis algorithm based on learning and searching methods in the field of Big Data to optimize TCM information management. We use TF-IDF theory in document clustering to cluster the name of Chinese Medical prescripts, construct word bag model, and combine universal hash with perfect hash, in order to establish a special key-value hashing band between Chinese Medical Data and diseases.
Cluster analysis is an essential tool in data mining. Several clustering algorithms have been proposed and implemented for which most are able to find the good quality or optimal clustering solutions. However, most of these algorithms still depend on the number of a cluster being provided a priori. In dealing with real-life problems, the number of clusters is unknown and determining the optimal number of clusters for a large density and high dimensionality dataset is quite a difficult task to handle. This paper, therefore, proposes five new hybrid symbiotic organism search algorithms to automatically partition datasets without any prior information regarding the number of clusters. Furthermore, the hybrid algorithms will be evaluated in terms of solution quality using the Davies–Bouldin clustering validity index. The simulation results show that the performance of the hybrid symbiotic organisms search particle swarm optimization algorithm is superior to the other proposed hybrid algorithms.
The crow is one of the most intelligent bird and infamous for observing other birds so that they can steal their food. The crow search algorithm (CSA), a nature-based optimizer, is inspired by the social behavior of crows. Scholars have applied the CSA to obtain efficient solutions to certain function and combinatorial optimization problems. Another popular and powerful method with several real-world applications (e.g., energy, finance, marketing, and medical imaging) is fuzzy clustering. The fuzzy c-means (FCM) algorithm is a critical fuzzy clustering approach given its efficiency and implementation easily. However, the FCM algorithm can be easily trapped in the local optima. To solve this data clustering problem, this study proposes a hybrid fuzzy clustering algorithm combines the CSA and a fireworks algorithm. The algorithm performance is evaluated using eight well-known UCI benchmarks. The experimental analysis concludes that adding the fireworks algorithm improved the CSA’s performance and offered better solutions than those by other meta-heuristic algorithms.
Cloud Computing offers storage resources as well as network and computing resources to the organizations. This eliminates the high infrastructure cost for the organizations that are using these services as they can now dynamically pay for these services, i.e., pay per use model, which is followed by most of the cloud providers. As the organization does not locally host these resources, these are comparatively far easier to manage and use than the traditional infrastructural resources. As a result of these factors, the popularity of cloud computing is increasing continuously. But this transfer of data and applications to the cloud server also creates some challenges. It poses problems that must be dealt with properly to ensure a secure cloud computing environment. As more and more sensitive data is being uploaded on the cloud in the present scenario, the privacy and security concerns associated with the data is continuously increasing. To address this, issue the data is stored on the cloud in the encrypted form. Also, as the amount of data stored is usually tremendous, so an efficient search scheme is also necessary. So here, we deal with two significant aspects of cloud computing: Encryption and Searching. We are proposing a secure and efficient encryption scheme to encrypt the data stored in the cloud as well as the queries along with a multi-keyword search scheme to search over the encrypted cloud data.
Since the advent of cloud computing, a huge number of data owners are outsourcing their sensitive data to be stored on to the cloud. To maintain privacy requirements, this sensitive data should be encrypted when it is stored on the cloud server which renders simple keyword based document retrieval schemes obsolete. In this paper, we propose a semantic multi-keyword ranked search scheme for document retrieval on cloud data that is encrypted. The proposed scheme returns not only the documents containing terms that match with our query terms but also some more documents that contain terms which are semantically similar to the query keywords. Our experimental result shows that our technique is more precise than TF-IDF/VSM(Vector Space Model) models that focus only keyword matching while simultaneously achieving faster retrieval times than other tree based TF-IDF/VSM approach as our algorithm runs on reduced dimensions.
Transportation is an important task in the society of today, because economies of the modern world is based on internal and foreign trade. It is obvious that optimized transportation can reduce amount of money spent daily on fuel, equipment and maintenance. Even small improvements can give a huge savings in absolute terms. A popular problem in the field of transportation is Vehicle Routing Problem (VRP). The general vehicle routing heuristics are an important research area as such heuristics are needed for real life problems and each new approach will continue a challenge to make even better heuristics for Vehicle Routing Problems. Tabu Search is still unpopular and rarely used algorithm. Therefore, the paper presents the idea to use Tabu Search algorithm to solve Vehicle Routing Problem with Time Windows constraint. The optimistic and interesting results of using the algorithms for benchmark cases were also presented in the paper. Next, these results were compared with world’s best values to show that the implementation of heuristic improved the best known solutions to benchmark cases for many problems.
Discrete optimization problems are often of practical real-world importance as well as computationally intractable. For example, the traveling salesperson, bin packing, and longest common subsequence problems are NP-Hard, as is resource constrained scheduling, and many single-machine scheduling problems (Garey & Johnson, 1979). Polynomial time algorithms for such problems are unlikely to exist, and the best known algorithms that guarantee optimal solutions have a worst-case exponential runtime. It is thus common to use stochastic local search and other metaheuristics (Gonzalez, 2018). Stochastic local search algorithms begin at a random search state, and apply a sequence of neighbor transitions to nearby search states. This includes perturbative (Hoos & Stützle, 2018) algorithms like simulated annealing (Delahaye, Chaimatanan, & Mongeau, 2019) and hill climbers (Hoos & Stützle, 2018), where each search state is a complete candidate feasible solution, and a mutation operator makes a small random modification to move to another local candidate solution; and also includes constructive (Hoos & Stützle, 2018) algorithms like stochastic samplers (Bresina, 1996; Cicirello & Smith, 2005; Grasas, Juan, Faulin, de Armas, & Ramalhinho, 2017; Langley, 1992; ReyesRubiano, Calvet, Juan, Faulin, & Bové, 2020), where each search state is a partial solution that is iteratively transformed into a complete solution. Stochastic local search algorithms do not guarantee optimal solutions. However, they often find near-optimal solutions in much less time than systematic search. They also offer an anytime property (Jesus, Liefooghe, Derbel, & Paquete, 2020; Zilberstein, 1996), where solution quality improves with runtime.
Quorum planted (<inline-formula> <tex-math notation="LaTeX">$l, d$ </tex-math></inline-formula>) motif search (qPMS) is a challenging computational problem in bioinformatics, mainly for the identification of regulatory elements such as transcription factor binding sites in DNA sequences. Large DNA datasets play an important role in identifying high-quality (<inline-formula> <tex-math notation="LaTeX">$l, d$ </tex-math></inline-formula>) motifs, while most existing qPMS algorithms are too time-consuming to complete the calculation of qPMS in a reasonable time. We propose an approximate qPMS algorithm called APMS to deal with large DNA datasets mainly by accelerating neighboring substring search and filtering redundant substrings. Experimental results on them show that APMS can not only identify the implanted (<inline-formula> <tex-math notation="LaTeX">$l, d$ </tex-math></inline-formula>) motifs, but also run orders of magnitude faster than the state-of-the-art qPMS algorithms. The source code of APMS and the python wrapper for the code are freely available at <uri>https://github.com/qyu071/apms</uri>.
In biological research, biology sequence alignment algorithm aims to find similarities between sequences. As the size of biological database increases exponentially, the complexity of sequence alignment process also increases rapidly, which results in a large amount of computational time. The Sunway TaihuLight is the world’s first heterogeneous supercomputer with peak performance over 100 PFlops and provides a new hardware platform for database search. In this paper we present an efficient method of protein database search based on Sunway TaihuLight supercomputer. Furthermore, we also optimize protein database search on Sunway TaihuLight to give full play to the performance of the SW26010 processor. In our proposed approach, we design hybrid sequence alignment by combining the Smith-Waterman local alignment algorithm and the Needleman-Wunsch global alignment algorithm. The protein database search is paralleled by message passing interface (MPI) and accelerated thread library (Athread). Experiment results with the Swiss-Prot database show that our implementation can effectively leverage the SW26010 processor’s special hardware architecture and achieve a speedup to 15.91 times on a single node. In addition, we expand the scale to 64 nodes to test the scalability of the parallel method on the Sunway TaihuLight system, and the results show that our parallel implementation of protein database search have a good expansibility and reliability.
Cloud computing is a type of parallel, configurable, and flexible system, which refers to the provision of applications on virtual data centers. However, reducing the energy consumption and also maintaining high computation capacity have become timely and important challenges. The concept of replication is used to face these challenges. By increasing the number of data replicas, the energy consumption, the performance, and also the cost of creating and maintaining new replicas also are increased. Deciding on the number of required replicas and their location on the cloud system is an NP‐hard problem. In this paper, the problem is formulated as an optimization problem and a hybrid metaheuristic algorithm is offered to solve it. The algorithm uses the global search capability of the Particle Swarm Optimization (PSO) algorithm and the local search capability of the Tabu Search (TS) to get high‐quality solutions. The efficiency of the method is shown by comparing it with simple PSO, TS, and Ant Colony Optimization (ACO) algorithm on different test cases. The obtained results indicate that the method outperforms all of them in terms of consumed energy and cost.
The coarse-grained multicomputer parallel model (CGM for short) has been used for solving several classes of dynamic programming problems. In this paper, we propose a parallel algorithm on the CGM model, with p processors, for solving the optimal binary search tree problem (OBST problem), which is a polyadic non-serial dynamic programming problem. Firstly, we propose a dynamic graph model for solving the OBST problem and show that each instance of this problem corresponds to a one-to-all shortest path problem in this graph. Secondly, we propose a CGM parallel algorithm based on our dynamic graph to solve the OBST problem through one-to-all shortest paths in this graph. It uses our new technique of irregular partitioning of the dynamic graph to try to bring a solution to the well-known contradictory objectives of the minimization of the communication time and the load balancing of the processors in this type of graph. Our solution is based on Knuth’s sequential solution and required $${\mathcal {O}}\left( \dfrac{n^2}{p} \right)$$On2p time steps per processor and $$\lceil \sqrt{2p} \rceil + k \times \left( {\left\lceil \dfrac{\lceil \sqrt{2p} \rceil }{2} \right\rceil } + 1 \right)$$⌈2p⌉+k×⌈2p⌉2+1 communication rounds. Integer k is a parameter used in the partitioning technique of our algorithm. This new CGM algorithm performs better than the previously most efficient solution, which uses regular partitioning of the tasks graph.
Abstract Short messages are one of the milestones on the web especially on social media (SM). Due to the widespread circulation of SM, it already turns into excessively painful capturing outmost relevant and significant information for certain users. One of the main motivations of this work is that many users may need an inclusive brief of all comments without reading the entire list of short messages for decision making. In this work, mining in big social media data is formulated for the first time into a multi-objective optimization (MOO) task to extract the essence of a text. Since some users may demand the brief at any moment, several groups of dissimilar short messages are established based on graph coloring mechanism. Six interesting feature are formalized to exhibit more interactive messages. A Gravitational Search Algorithm (GSA) is employed to satisfy several important objectives for generating a concise summary. The problem was picked by using the Normal Boundary Intersection (NBI) mechanism to trade-off among different features. Additionally, to satisfy real-time needs, an inventive incremental grouping task is modelled to update the existing colors. From exhaustive experimental results, the proposed approach outperformed other strong comparative methods.
In the preceding video compression method, some limitations required to be enhanced, i.e., the compression ratio has to be improved. To enhance the conventional method negative aspect, an innovative video compression method employing diamond search depend integrated projection error measurement (DIPEM) estimation algorithm is anticipated. To begin with, the input video frames are processed by employing watershed algorithm and then the video frames motion vectors are evaluated by diamond search dependent integrated projection error motion inference algorithm. Subsequent the motion vectors assessment, encoding and decoding process are performed by JPEG-LS method. The execution result depicts the efficiency of anticipated method, in compressing more number of videos and the performance is assessed with conventional video compression techniques. The comparison result demonstrates that our anticipated method acquires high-quality compression ratio and PSNR than the conventional techniques.
Distributed computing gives the gigantic capacity ability to the clients to send their applications without any infrastructure investment. Based on the application lot of intermediate dataset will be created. To protecting these intermediate dataset is a challenging task. Moreover, encrypting all dataset is a time and cost consuming. To overcome the problem, in this paper we proposed a privacy preserving of intermediate dataset using a combination of oppositional gravitational search algorithm and elliptic curve cryptography (OGSA + ECC). Initially, we split the dataset into a number of the intermediate datasets, then, we choose the node corresponding intermediate dataset from the cloud using an oppositional gravitational search algorithm (OGSA). After that, we choose the sensitive data from the dataset using the information gain measure to minimise the processing time and cost. Then, using the ECC algorithm the sensitive data is encrypted and in the cloud the secure data are stored. The experimentation is carried out in terms of encryption time and memory use.

{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# MCIS6273 Data Mining (Prof. Maull) / Fall 2021 / HW3b\n", "\n", "**This assignment is worth up to 10 POINTS to your grade total if you complete it on time.**\n", "\n", "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n", "|:---------------:|:--------:|:---------------:|\n", "| 10 | Wednesday, Devember 1  @ Midnight | _up to_ 8 hours |\n", "\n", "\n", "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n", "\n", "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n", "\n", "## OBJECTIVES\n", "* Perform Bayesian text classification\n", "\n", "## WHAT TO TURN IN\n", "You are being encouraged to turn the assignment in using the provided\n", "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n", "`homework/hwN`.   Put all of your files in that directory.  Then zip that directory,\n", "rename it with your name as the first part of the filename (e.g. `maull_hwN_files.zip`), then\n", "download it to your local machine, then upload the `.zip` to Blackboard.\n", "\n", "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n", "on the basics of using zip in Linux.\n", "\n", "If you choose not to use the provided notebook, you will still need to turn in a\n", "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n", "this homework.\n", "\n", "\n", "## ASSIGNMENT TASKS\n", "### (100%) Perform Bayesian text classification \n", "\n", "Text classification is an important application area of machine learning.  Indeed, the early\n", "advances in the field were in text and image processing.  We are now beneficiaries of the\n", "libraries and modules that provide us the foundation for a variety of techniques to do\n", "sophisticated text analytics and process withuout much effort.\n", "\n", "With text classification, one goal we might like to accomplish is determine the origin of\n", "particular text.  What once used to be the arena of computational linguists and computer\n", "scientists, is now growing in [computational digital humanities](https://jitp.commons.gc.cuny.edu/a-survey-of-digital-humanities-programs/),\n", "but is [not without issues](https://www.chronicle.com/article/the-digital-humanities-debacle/). In \n", "this assignment we are going to use Bayesian techniques to process a corpus, or body of text, with the expressed\n", "goal of classifying it.  In fact, we're going to take multiple texts and generate a classifier\n", "that (with some work), will be able to distinguish between multiple topics.\n", "\n", "As a graduate student, you are fully aware of the extent of academic research represented\n", "by the multitude of disciplines in our university community.  You are likely well aware\n", "of the many thousands of academic journals that contain the intellectual products of \n", "the research in those disciplines. We are going to build a Bayesian classifier that\n", "will be trained on the text from abstracts of academic papers, and be able to \n", "classify unseen (unlabeled) abstracts into their corresponding disciplines.\n", "\n", "For this assignment we are going to keep it simple, mostly just to get you started on \n", "the using technique so that you might extend it and learn ways to improve it in the future.\n", "\n", "Laying out the intuition of the technique, let's abstractly think about the problem at hand. An\n", "academic discipline usually contains a large domain-specific vocabulary that make it unique \n", "relative to\n", "other disciplines.  Think of the words in the vocabulary as a \"profile\" (loosely speaking) of the \n", "discipline.  More\n", "concretely, the probability that computer science papers used a word like \"algorithm\" is much higher than\n", "the probability of \"algorithm\" being used in an education paper.  Ultimately with enough examples of the\n", "writings of a particular discipline, the easier it would be to establish the probabilities of certain\n", "words, phrases and even punctuation usage.  While we are going to choose to classify papers, the\n", "same technique could be used to classify authors, for example, classifying texts written  by\n", "_Don Knuth_ versus texts authored by _Noam Chomsky_.\n", "\n", "Bayesian techniques are a mainstain in text classification of all kinds, and the ease with which\n", "Bayes classifiers can be trained make it a technique that can be fast to implement and get \n", "results that are often very accurate.\n", "\n", "In the interest of time and resouces, we're going to develop a simple Bayesian text classifier to distinguish\n", "between the writings of five disciplines: _sociology_, _education_, _physics_, _computer science_ and \n", "_economics_.\n", "\n", "Under ordinary circumstances we would like to have as many documents from each of these disciplines\n", "as possible, and as you might know, obtaining full text documents is often difficult or requires \n", "extraction of raw text from PDF documents (that are often only obtained under publisher license or \n", "may require payment).  For more information about how extract text from PDFs in\n", "Python, please see the [pdfminer.six module](https://github.com/pdfminer/pdfminer.six) as it \n", "contains a number of wonderful functionalities to get the text portion of a PDF so it can\n", "be processed by more common text and string processing tools.\n", "\n", "Instead of full text PDF documents, we are lucky in academia to have _abstracts_ which \n", "summarize the paper for the reader. These usually paragraph-long text are often enough \n", "to get a good idea about the thrust of the paper, and as we have learned in this class\n", "_large amounts_ of data are often necessary for any meaningful test of an algorithm. \n", "\n", "In class we talked about test sets, training sets and the permutations we might conduct to\n", "get a mix of test/train sets to build supervised learning algorithms.  You are being\n", "provided with a small set of testing and training documents for each discipline, and  will use\n", "these documents and the provided Jupyter Notebook that step through the process\n", "of building the na&iuml;ve Bayes classifier with Sci-Kit Learn's algorithms.\n", "\n", "We would like to have as many documents  as possible -- but in the spirit\n", "of time, we will instead use tens of abstracts from each discipline and those\n", "will act as the training corpus.  In a more robust classifier training, \n", "we will want hundreds or even thousands of abstracts, and as you will see\n", "this will impact the results as well as bias the classifier.\n", "\n", "## Document Processing: A Very Short Primer\n", "\n", "At the heart of document classification is the _model_ for document features.  One popular model is\n", "the TF-IDF or Term Frequency Inverse Document Frequency.  The intuition behind analyzing words in\n", "documents hinges on the following:\n", "\n", "* terms that are frequent _in documents_ are given higher importance than those that are infrequent,\n", "* terms that are frequent _across_ documents are not considered as important;\n", "\n", "that is _common_ words across an entire corpus are *discounted* while\n", "those that are _common_ within documents are *boosted*.  This is an effective way to differentiate since\n", "the intuition that the things that make your writing unique are amplified, while those that are not\n", "differentiators will count less.\n", "\n", "To realize the TF-IDF, we will need to break apart the two components TF (or **term frequency**) and\n", " IDF (**inverse document frequency**) and then conjoin them.\n", "\n", "**Term frequency (TF)** is a simple concept and is exactly as it says: the _counts_ of terms in a document.\n", "So for a term (word) $t$ and document $d$, the TF is just the number of occurences of $t$ in $d$,\n", "\n", "$$\\textrm{tf}(t,d) = \\big| t \\in d \\big|$$\n", "\n", "**Inverse document frequency (IDF)** provides a way to determine if a terms is rare or\n", "common given _all_ documents $D$, and is logarithmically scaled so rare terms avoid completely disappearing.  Thus,\n", "\n", "$$\n", "\\textrm{idf}(t,D) = \\frac{\\big| D \\big|}{ 1 + \\big| \\{t \\in d | d \\in D \\} \\big| }\n", "$$\n", "\n", "**TF-IDF** is thus: for a set of documents (corpus) $D$ and document $d \\in D$ and terms $t \\in d$,\n", "\n", "\n", "$$\n", "\\textrm{tfidf}(t,d,D)= \\textrm{tf}(t,d,D) \\times \\textrm{idf}(t,D)\n", "$$\n", "\n", "\n", "Luckily, `sklearn` implements TF-IDF for us in the [`sklearn.feature_extraction.text.TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=vectorizer#sklearn.feature_extraction.text.TfidfVectorizer)\n", "class.\n", "The underlying implementation uses the words as the feature matrix where the TF-IDF is computed over\n", "every document input to the [`vectorizer.fit_transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=vectorizer#sklearn.feature_extraction.text.TfidfVectorizer.transform)\n", "method.\n", "\n", "Now that we've implemented to the primary machinery of the method, let's bring back Bayesian.  Recall the\n", "Bayesian method:\n", "\n", "$$ \\Pr(C \\big| w_1, \\ldots, w_n) = \\Pr( C ) \\prod_i^n \\Pr(  w_i \\big| C ) $$\n", "\n", "where $C$ is the document class (Plato or class `A`, Hume or class `B` and Aristotle or class `C`) and $w_i$\n", "the words in the document.  Concretely, a document $D_i$ has some probability $P_i$ based on the\n", "occurrence of the words $w_i$ in that document, and that a classifier will decide the class $\\hat{C}$  of document\n", "$D_i$ by computing\n", "\n", "$$ \\hat{C} = \\mathrm{argmax}_C \\Pr( C ) \\prod_i^n \\Pr(  w_i \\big| C )$$\n", "\n", "by training the classifier on some labeled data.  Once trained the classifier can be tested and then used on\n", "unlabelled data to classify the author.  While this exercise is decidely oversimplified (we'd not really be all\n", "that interested in classifying the works of only 5 disciplines over a narrow number of test instances),\n", "you can extend this to other domains where perhaps you're not classifying topics, but styles, tone or even document complexity.\n", "\n", "Completing the assignment will require you use the provided notebook and corresponding data files.\n", "This notebook can be found in `example_notebook.ipynb`.  Study it closely.\n", "\n", "&#167;  Using the notebook provided and corresponding files, execute the notebook\n", "to load the training data.  \n", "\n", "You will do this by uncommenting the cell that sets the `file_list` for\n", "loading the corpori.  Once you execute this cell the classifier will\n", "be trained and you can then test it.\n", "\n", "* **You will just need to show the uncommented code and initialization of the classifier\n", "in this step.**  \n", "* Open the files in `data/train` and explore their contents.  You will notice these are\n", "just large numbers of words that have come directly from the abstracts.  Later we \n", "will show how to build your own.\n", "\n", "\n", "&#167;  Now that you have a classifier, the real work is to be done -- testing.  I have\n", "provided a set of test documents in the `data/test/` folder.  \n", "\n", "Write a cell that loads a the list of documents in the `data/test` folder\n", "and passes that list to the function `vectorizer.transform()` (this will\n", "be the same `vectorizer` in the prior cell).  See the example notebook\n", "for more extensive information on how you might do this.\n", "\n", "\n", "&#167;  Now that you have a vectorizer and a classifier trained and tested\n", "you might notice that the classifier is not that great just yet\n", "on the limited data.\n", "\n", "You might have noticed that sociology, computer science and eduction\n", "are seemingly similar.  Let's try to do better by expanding the computer\n", "science corpus.  \n", "\n", "You will see a file in `data` called `seed_doi_compsci`.  In it are\n", "over 100 DOIs to a more extensive computer science corpus.  A\n", "DOI or Digital Object Identifier, is permanent identified for \n", "an academic paper (or technically any digital asset).  It allows\n", "any machine to lookup the _metadata_ of the DOI and then also\n", "resolve the actual object, whether it is a PDF, dataset, software \n", "or whatever it resolves to.\n", "\n", "There is a service called [Semantic Scholar](https://semanticscholar.org)\n", "which provides an academic service for research papers and their networked connections \n", "to other relevant papers and authors, and a host of relevant metadata\n", "through an API that provides abstracts and other metadata from a single\n", "service, which would otherwise be difficult to obtain.  Another\n", "service [crossref](https://api.crossref.org/swagger-ui/index.html) provides similar API \n", "support for publication metadata and the two services complement each other.\n", "\n", "We're going to now have some fun with APIs and extract the abstracts\n", "for all of those DOIs in the seed file, then retrain the classifier.\n", "\n", "You can use the API as provided in the [semantic scholar documentation](https://www.semanticscholar.org/product/api)\n", "and in particular you will use the lookup functions provided by\n", "the API to lookup by DOI.  Once you have the JSON object back,\n", "you can grab the abstract from that.  \n", "\n", "You are also free to install the [Semantic Scholar Python \n", "package from PyPi](https://pypi.org/project/semanticscholar/) which\n", "hides most of the processing and API calls and just provides\n", "the essential functions to process the return object in a natural\n", "programmatic way.  I have used the package and it is very good.\n", "\n", "**NOTE:** _Semantic Scholar rate limits your calls to 100 per 5 minutes or 1 every\n", "3 seconds / 20 per minute.  You will want to use the Python `time.sleep(n_sec)`\n", "function to avoid being shut down for API timeout / cooloff.  Please be nice\n", "and usually I set the sleep to 5 seconds, just to be safe._\n", "\n", "\n", "In your notebook do the following:\n", "\n", "1. **Write a function to take as input DOIs in `seed_doi_compsci` and produces\n", "a new file in the `data/train` folder called `compsci_extended.txt`.**  This file will\n", "just contain the contentation of all the abstracts. You will use 75 of the abstracts\n", "as the test set and the remaining as the training set.\n", "\n", "\n", "\n", "1. **Retrain the classifier with the extended corpus instead of the original one.**\n", "\n", "1. Test the new classifier on the remaining DOIs which you did not train.  Please show the\n", "outcome of the test.\n", "\n", "1. How well did the new classifier do on the test data?  In your reporting of this number, \n", "list the percent of correctly classified documents (e.g. a perfect classifer will be 1.00 or 100%, \n", "and a poor classifer might be 0.333 or 33.3%).\n", "\n", "\n", "&#167;  The classifier `predict` method only returns the label, but you can get the probabilities assigned to all\n", "classes using [`predict_proba()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB.predict_proba).\n", "\n", "**Answer the following questions inside your notebook:**\n", "\n", "1. Make an observation about the class probabilities.  What did you notice?\n", "2. Provide some commentary as a thought exercise or if you have time, provide some example code).\n", "\n", "\n", "&#167;  A bonus assignment that extends this analysis will be provided for you to attempt.\n", "\n", "\n", "\n"]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python [default]", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "12px", "width": "252px"}, "navigate_menu": true, "number_sections": false, "sideBar": true, "threshold": "1", "toc_cell": false, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 0}
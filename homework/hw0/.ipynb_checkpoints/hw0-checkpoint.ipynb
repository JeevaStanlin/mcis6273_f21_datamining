{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCIS6273 Data Mining (Prof. Maull) / Fall 2021 / HW0\n",
    "\n",
    "**This assignment is worth up to 20 POINTS to your grade total if you complete it on time.**\n",
    "\n",
    "| Points <br/>Possible | Due Date | Time Commitment <br/>(estimated) |\n",
    "|:---------------:|:--------:|:---------------:|\n",
    "| 20 | Wednesday, Sep 1 @ Midnight | _up to_ 20 hours |\n",
    "\n",
    "\n",
    "* **GRADING:** Grading will be aligned with the completeness of the objectives.\n",
    "\n",
    "* **INDEPENDENT WORK:** Copying, cheating, plagiarism  and academic dishonesty _are not tolerated_ by University or course policy.  Please see the syllabus for the full departmental and University statement on the academic code of honor.\n",
    "\n",
    "## OBJECTIVES\n",
    "* Familiarize yourself with the JupyterLab environment, Markdown and Python\n",
    "\n",
    "* Familiarize yourself with Github and basic git\n",
    "\n",
    "* Explore JupyterHub Linux console integrating what you learned in the prior parts of this homework\n",
    "\n",
    "* Listen to the Talk Python['Podcast'] from June 25, 2021: A Path to Data Science Interview with Sanyam Bhutani\n",
    "\n",
    "* Explore Python for data munging and analysis, with an introduction to CSV and Pandas\n",
    "\n",
    "## WHAT TO TURN IN\n",
    "You are being encouraged to turn the assignment in using the provided\n",
    "Jupyter Notebook.  To do so, make a directory in your Lab environment called\n",
    "`homework/hw0`.   Put all of your files in that directory.  Then zip that directory,\n",
    "rename it with your name as the first part of the filename (e.g. `maull_hw0_files.zip`), then\n",
    "download it to your local machine, then upload the `.zip` to Blackboard.\n",
    "\n",
    "If you do not know how to do this, please ask, or visit one of the many tutorials out there\n",
    "on the basics of using zip in Linux.\n",
    "\n",
    "If you choose not to use the provided notebook, you will still need to turn in a\n",
    "`.ipynb` Jupyter Notebook and corresponding files according to the instructions in\n",
    "this homework.\n",
    "\n",
    "\n",
    "## ASSIGNMENT TASKS\n",
    "### (0%) Familiarize yourself with the JupyterLab environment, Markdown and Python \n",
    "\n",
    "As stated in the course announcement [Jupyter (https://jupyter.org)](https://jupyter.org) is the\n",
    "core platform we will be using in this course and\n",
    "is a popular platform for data scientists around the world.  We have a JupyterLab\n",
    "setup for this course so that we can operate in a cloud-hosted environment, free from\n",
    "some of the resource constraints of running Jupyter on your local machine (though you are free to set\n",
    "it up on your own and seek my advice if you desire).\n",
    "\n",
    "You have been given the information about the  Jupyter environment we have setup for our course, and\n",
    "the underlying Python environment will be using is the [Anaconda (https://anaconda.com)](https://anaconda.com)\n",
    "distribution.  It is not necessary for this assignment, but you are free to look at the multitude\n",
    "of packages installed with Anaconda, though we will not use the majority of them explicitly.\n",
    "\n",
    "As you will soon find out, Notebooks are an incredibly effective way to mix code with narrative\n",
    "and you can create cells that are entirely code or entirely Markdown.  Markdown (MD or `md`) is\n",
    "a highly readable text format that allows for easy documentation of text files, while allowing\n",
    "for HTML-based rendering of the text in a way that is style-independent.\n",
    "\n",
    "We will be using Markdown frequently in this course, and you will learn that there are many different\n",
    "\"flavors\" or Markdown.  We will only be using the basic flavor, but you will benefit from exploring\n",
    "the \"Github flavored\" Markdown, though you will not be responsible for using it in this course -- only the\n",
    "\"basic\" flavor.  Please refer to the original course announcement about Markdown.\n",
    "\n",
    "&#167;  **THERE IS NOTHING TO TURN IN FOR THIS PART.** Play with and become familiar with the basic functions of\n",
    "the Lab environment given to you online in the course Blackboard.\n",
    "\n",
    "\n",
    "&#167;  **PLEASE _CREATE A MARKDOWN DOCUMENT_ CALLED `semester_goals.md` WITH 3 SENTENCES/FRAGMENTS THAT\n",
    "ANSWER THE FOLLOWING QUESTION:**\n",
    "\n",
    "* **What do you wish to accomplish this semester in Data Mining?**\n",
    "\n",
    "Read the documentation for basic Markdown [here](https://www.markdownguide.org/basic-syntax). \n",
    "Turn in the text `.md` file *not* the processed `.html`.  In whatever you turn in, \n",
    "you must show the use of *ALL* the following:\n",
    "\n",
    "* headings (one level is fine),\n",
    "* bullets,\n",
    "* bold and italics\n",
    "\n",
    "Again, the content of your document needs to address the question above and it should live\n",
    "in the top level directory of your assignment submission.  This part will be graded but no\n",
    "points are awarded for your answer.\n",
    "\n",
    "\n",
    "\n",
    "### (0%) Familiarize yourself with Github and basic git \n",
    "\n",
    "[Github (https://github.com)](https://github.com) is the _de facto_ platform for open source software in the world based\n",
    "on the very popular [git (https://git-scm.org)](https://git-scm.org) version control system. Git has a sophisticated set\n",
    "of tools for version control based on the concept of local repositories for fast commits and remote\n",
    "repositories only when collaboration and remote synchronization is necessary.  Github enhances git by providing\n",
    "tools and online hosting of public and private repositories to encourage and promote sharing and collaboration.\n",
    "Github hosts some of the world's most widely used open source software.\n",
    "\n",
    "**If you are already familiar with git and Github, then this part will be very easy!**\n",
    "\n",
    "&#167;  **CREATE A PUBLIC GITHUB REPO NAMED `\"mcis6273-F21-datamining\"` AND PLACE A README.MD FILE IN IT.**\n",
    "Create your first file called\n",
    "`README.md` at the top level of the repository.  You can put whatever text you like in the file \n",
    "(If you like, use something like [lorem ipsum](https://lipsum.com/)\n",
    "to generate random sentences to place in the file.).\n",
    "Please include the link to **your** Github repository that now includes the minimal `README.md`. \n",
    "You don't have to have anything elaborate in that file or the repo. \n",
    "\n",
    "\n",
    "\n",
    "### (0%) Explore JupyterHub Linux console integrating what you learned in the prior parts of this homework \n",
    "\n",
    "The Linux console in JupyterLab is a great way to perform command-line tasks and is an essential tool\n",
    "for basic scripting that is part of a data scientist's toolkit.  Open a console in the lab environment\n",
    "and familiarize yourself with your files and basic commands using git as indicated below.\n",
    "\n",
    "1. In a new JupyterLab command line console, run the `git clone` command to clone the new\n",
    "  repository you created in the prior part.\n",
    "  You will want to read the documentation on this \n",
    "  command (try here [https://www.git-scm.com/docs/git-clone](https://www.git-scm.com/docs/git-clone) to get a good\n",
    "  start).\n",
    "2. Within the same console, modify your `README.md` file, check it in and push it back to your repository, using\n",
    "  `git push`.  Read the [documentation about `git push`](https://git-scm.com/docs/git-push).\n",
    "3. The commands `wget` and `curl` are useful for grabbing data and files from remote resources off the web.\n",
    "  Read the documentation on each of these commands by typing `man wget` or `man curl` in the terminal.\n",
    "  Make sure you pipe the output to a file or use the proper flags to do so.\n",
    "\n",
    "&#167;  **THERE IS NOTHING TO TURN IN FOR THIS PART.**\n",
    "\n",
    "\n",
    "\n",
    "### (30%) Listen to the Talk Python['Podcast'] from June 25, 2021: A Path to Data Science Interview with Sanyam Bhutani \n",
    "\n",
    "Data science is one of the most important and \"hot\" disciplines today\n",
    "and there is a lot going on from data engineering to modeling and\n",
    "analysis.\n",
    "\n",
    "Bhutani is one of the top [Kaggle]() leaders and in this interview\n",
    "shares his experience from computer science to data science, \n",
    "documenting some of the lessons he learned along the way.\n",
    "\n",
    "Please listen to this one hour podcast and answer some of the questions below.\n",
    "You can listen to it from one of the two links below:\n",
    "\n",
    "* [Talk Python['Podcast'] landing page](https://talkpython.fm/episodes/transcript/322/a-path-into-data-science)\n",
    "* [direct link to mp3 file](https://downloads.talkpython.fm/podcasts/talkpython/322-starting-in-data-sci.mp3)\n",
    "\n",
    "&#167;  **PLEASE ANSWER THE FOLLOWING QUESTIONS AFTER LISTENING TO THE PODCAST:**\n",
    "\n",
    "1. List 3 things that you learned from this podcast?\n",
    "\n",
    "2. What is your reaction to the podcast?  Pick at least one point Sanyam brought up in the interview that you agree with and list your reason why.\n",
    "\n",
    "3. After listening to the podcast, do you think you are more interested or less interested in a career in Data Science?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWERS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. List 3 things that you learned from this podcast?**\n",
    "1. Top down approach - Give Youself a deadline, learn the basics and start developing your first website/app , then enhance the website/app with the learning you get everyday\n",
    "2. Make Progress in learning. Keep learning new stuff everyday.\n",
    "3. Join a community where you can learn and contribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is your reaction to the podcast?  Pick at least one point Sanyam brought up in the interview that you agree with and list your reason why.**\n",
    "The podcast was really informative and interesting. I was able to get a lot of new informations including Kaggle. I agree with the disconnect in university learning and actual working environment that Sanyam was talking about. Most of the times university learnings are good for getting good grades and serves as a pre-requisite for getting jobs. I was able to successfully work in the field of datawarehousing though I had very few subjects related to computer science in my undergraduate course. I can very well relate to the top-down approach that Sanyam was talking about. Once we are into it, we start exploring various ways to learn from the existing or create our own new things. Continuous learning and exploring will definitely help in our journey as a data scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. After listening to the podcast, do you think you are more interested or less interested in a career in Data Science?**\n",
    "This podcast has definitely kindled my interest in data science and I am looking forward for a career in Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (70%) Explore Python for data munging and analysis, with an introduction to CSV and Pandas \n",
    "\n",
    "\n",
    "Python's strengths shine when tasked with data munging and analysis.  As we will learn throughout\n",
    "the course, there are a number of excellent data sources for open data of all kinds now\n",
    "available for the public.  These open data sources are heralding the new era of transparency\n",
    "from all levels from small municipal data to big government data, from transportation, to science,\n",
    "to education.\n",
    "\n",
    "To warm up to such datasets, we will be working with an interesting\n",
    "dataset from the US Fish and Wildlife Service (FWS).  This is a \n",
    "water quality data set taken from a managed national refuge in\n",
    "Virginia called Back Bay National Wildlife Refuge, which was \n",
    "established in 1938.  As a function of being managed by the FWS, \n",
    "water quality samples are taken regularly from the marshes within \n",
    "the refuge.\n",
    "\n",
    "You can (and should) learn a little more about Back Bay from\n",
    "this link, since it has an interesting history, features and wildlife.\n",
    "\n",
    "* [https://www.fws.gov/refuge/Back_Bay/about.html](https://www.fws.gov/refuge/Back_Bay/about.html)\n",
    "\n",
    "\n",
    "The data we will be looking at can be found as a direct download \n",
    "from data.gov, the US data repository where many datasets from\n",
    "a variety of sources can be found -- mostly related to the \n",
    "multitude of US government agencies.\n",
    "\n",
    "The dataset is a small water quality dataset with several decades\n",
    "of water quality data from Back Bay.  We will be warming up\n",
    "to this dataset with a basic investigation into the shape, content\n",
    "and context of the data contained therein.\n",
    "\n",
    "\n",
    "In this part of the assignment, we will make use of Python libraries to pull the data from the\n",
    "endpoint and use [Pandas](https://pandas.pydata.org) to plot the data.  The raw CSV data is\n",
    "readily imported into Pandas from the following URL:\n",
    "\n",
    "* [FWS Water Quality Data 12/20/2020](https://catalog.data.gov/dataset/water-quality-data/resource/f4d736fd-ade9-4e3f-b8e0-ae7fd98b2f87)\n",
    "\n",
    "Please take a look at the page, on it you will notice a link \n",
    "to the raw CSV file:\n",
    "\n",
    "* [https://ecos.fws.gov/ServCat/DownloadFile/173741?Reference=117348](https://ecos.fws.gov/ServCat/DownloadFile/173741?Reference=117348)\n",
    "\n",
    "We are going to explore this dataset to learn a bit more about the \n",
    "water quality characteristics of Bay Bay over the past couple decades\n",
    "or so.\n",
    "\n",
    "&#167;  **WRITE THE CODE IN YOUR NOTEBOOK TO LOAD AND RESHAPE THE COMPLETE CSV WATER QUALITY DATASET**:\n",
    "\n",
    "You will need to perform the following steps:\n",
    "\n",
    "1. **use [`pandas.read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) method to load the dataset** into a Pandas DataFrame;\n",
    "2. **clean the data so that the range of years is restricted to the 20 year period from 1999 to 2018**\n",
    "5. **store the entire dataset back into a new CSV** file called `back_bay_1998-2018_clean.csv`.\n",
    "\n",
    "**HINTS:** _Here are some a code hints you might like to study and use to craft a solution:_\n",
    "\n",
    "* study  [`pandas.DataFrame.query()]`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query) to learn how to filter and query year ranges\n",
    "* study  [`pandas.DataFrame.groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html?highlight=groupby#pandas.DataFrame.groupby) to understand how to group data\n",
    "\n",
    "\n",
    "&#167;  **USE PANDAS TO LOAD THE CSV DATA TO A DATAFRAME AND ANSWER THE FOLLOWING QUESTIONS:**\n",
    "\n",
    "1. How many and what are the names of the columns in this dataset?\n",
    "2. What is the mean `Dissolved Oxygen (mg/L)` over the entire dataset?\n",
    "3. Which year were the highest number of `AirTemp (C)` data points collected?\n",
    "4. Which year were the least number of `AirTemp (C)` data points collected?\n",
    "\n",
    "\n",
    "To answer these questions, you'll need to dive further into Pandas, which is\n",
    "the standard tool in the Python data science stack for loading, manipulating,\n",
    "transforming, analyzing and preparing data as input to other tools such as\n",
    "[Numpy (http://www.numpy.org/)](http://www.numpy.org/), \n",
    "[SciKitLearn (http://scikit-learn.org/stable/index.html)](http://scikit-learn.org/stable/index.html), \n",
    "[NLTK (http://www.nltk.org/)](http://www.nltk.org/) and others.\n",
    "\n",
    "For this assignment, you will only need to learn how to load and select data using Pandas.\n",
    "\n",
    "* **LOADING DATA**\n",
    "The core data structure in Pandas is the `DataFrame`.  You will need to visit\n",
    "the Pandas documentation [(https://pandas.pydata.org/pandas-docs/stable/reference/)](https://pandas.pydata.org/pandas-docs/stable/reference/)\n",
    "to learn more about the library, but to help you along with a hint, read the\n",
    "documentation on the [`pandas.read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) method.\n",
    "\n",
    "* **SELECTING DATA**\n",
    "The [tutorial here on indexing and selecting](http://pandas.pydata.org/pandas-docs/stable/indexing.html)\n",
    "should be of great use in understanding how to index and select subsets of\n",
    "the data to answer the questions.\n",
    "\n",
    "* **GROUPING DATA** You may use [`DataFrame.value_counts()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html?highlight=value_counts#pandas.DataFrame.value_counts) or [`DataFrame.groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html) to group\n",
    "the data you need for these questons.  You will also find [`DataFrame.groupby()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html?highlight=groupby#pandas.DataFrame.groupby) and [`DataFrame.describe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html?highlight=describe#pandas.DataFrame.describe) very useful.\n",
    "\n",
    "**CODE HINTS**\n",
    "\n",
    "Here is example code that should give you clues about the structure\n",
    "of your code for this part.\n",
    "\n",
    "```python\n",
    "  import pandas as pd\n",
    "\n",
    "  df = pd.read_csv('your_json_file.csv')\n",
    "\n",
    "  # code for question 1 ... and so on\n",
    "```\n",
    "\n",
    "\n",
    "&#167;  **EXPLORING WATER SALINITY IN THE DATA**\n",
    "\n",
    "The Back Bay refuge is on the eastern coast of Virginia and to\n",
    "the east is the Atlantic Ocean.  Salinity is a measure of\n",
    "the salt concentration of water, and you can learn a little more\n",
    "about salinity in water [here](https://www.usgs.gov/special-topic/water-science-school/science/saline-water-and-salinity?qt-science_center_objects=0#qt-science_center_objects).\n",
    "\n",
    "You will notice that there is a `Site_Id` variable in the data, which \n",
    "we will find refers to the five sampling locations (see the [documentation here](https://ecos.fws.gov/ServCat/Reference/Profile/117348))\n",
    "of (1) the Bay, (2) D-Pool (fishing pond), (3) C-Pool, (4) B-Pool and (5) A-Pool.\n",
    "\n",
    "The ppt in Salinity is the percent salinity, and so 1 ppt is equivalent to 10000 ppm salinity.  Use this information to answer \n",
    "the following questions.\n",
    "\n",
    "1. Which sampling location has the highest mean ppt?  What is the equivalent ppm?\n",
    "2. When looking at the mean ppt, which location would you infer is furthest from the influence of ocean water inflows? \n",
    "   (Assume that higher salinity correlates to closer proximity to the ocean.)\n",
    "3. Dig a little deeper into #2, and write why there may be some uncertainty in your answer? (hint: certainty is improved by consistency in data)\n",
    "4. Use the data to determine the correlation between `Salinity (ppt)` and `pH (standard units)`.  Use the [DataFrame.corr()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.corr.html?highlight=correlate).  You just need to report the correlation value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWERS:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Use pandas.read_csv() method to load the dataset into a Pandas DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/jovyan/mcis6273_f21_datamining/homework/hw0/BKB_WaterQualityData_2020084.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site_Id</th>\n",
       "      <th>Unit_Id</th>\n",
       "      <th>Read_Date</th>\n",
       "      <th>Salinity (ppt)</th>\n",
       "      <th>Dissolved Oxygen (mg/L)</th>\n",
       "      <th>pH (standard units)</th>\n",
       "      <th>Secchi Depth (m)</th>\n",
       "      <th>Water Depth (m)</th>\n",
       "      <th>Water Temp (?C)</th>\n",
       "      <th>Air Temp-Celsius</th>\n",
       "      <th>Air Temp (?F)</th>\n",
       "      <th>Time (24:00)</th>\n",
       "      <th>Field_Tech</th>\n",
       "      <th>DateVerified</th>\n",
       "      <th>WhoVerified</th>\n",
       "      <th>AirTemp (C)</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/3/1994</td>\n",
       "      <td>1.3</td>\n",
       "      <td>11.7</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5.9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>46.40</td>\n",
       "      <td>11:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/31/1994</td>\n",
       "      <td>1.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.35</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>36.68</td>\n",
       "      <td>11:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/7/1994</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.60</td>\n",
       "      <td>5.9</td>\n",
       "      <td>7.6</td>\n",
       "      <td>45.68</td>\n",
       "      <td>9:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/23/1994</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>36.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/28/1994</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.00</td>\n",
       "      <td>10:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/11/2018</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.00</td>\n",
       "      <td>9:30</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>25.555556</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/24/2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.00</td>\n",
       "      <td>9:30</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>14.444444</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/28/2018</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.90</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.00</td>\n",
       "      <td>9:20</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>9.444444</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/7/2018</td>\n",
       "      <td>1.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.90</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.00</td>\n",
       "      <td>9:45</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>18.333333</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/11/2018</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.00</td>\n",
       "      <td>9:40</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2371 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Site_Id Unit_Id   Read_Date  Salinity (ppt)  Dissolved Oxygen (mg/L)  \\\n",
       "0        Bay     NaN    1/3/1994             1.3                     11.7   \n",
       "1        Bay     NaN   1/31/1994             1.5                     12.0   \n",
       "2        Bay     NaN    2/7/1994             1.0                     10.5   \n",
       "3        Bay     NaN   2/23/1994             1.0                     10.1   \n",
       "4        Bay     NaN   2/28/1994             1.0                     12.6   \n",
       "...      ...     ...         ...             ...                      ...   \n",
       "2366     Bay     NaN  10/11/2018             1.9                      5.0   \n",
       "2367     Bay     NaN  10/24/2018             0.0                      9.0   \n",
       "2368     Bay     NaN  10/28/2018             0.9                      2.9   \n",
       "2369     Bay     NaN   11/7/2018             1.7                      NaN   \n",
       "2370     Bay     NaN  12/11/2018             0.1                      NaN   \n",
       "\n",
       "      pH (standard units)  Secchi Depth (m)  Water Depth (m)  Water Temp (?C)  \\\n",
       "0                     7.3              0.40             0.40              5.9   \n",
       "1                     7.4              0.20             0.35              3.0   \n",
       "2                     7.2              0.25             0.60              5.9   \n",
       "3                     7.4              0.35             0.50             10.0   \n",
       "4                     7.2              0.20             0.40              1.6   \n",
       "...                   ...               ...              ...              ...   \n",
       "2366                  7.0              4.00             1.20             25.0   \n",
       "2367                  7.0              0.30             0.60             18.0   \n",
       "2368                  7.0              0.40             0.90             13.0   \n",
       "2369                  7.0              0.45             0.90             20.0   \n",
       "2370                  7.0              0.10             0.10             10.0   \n",
       "\n",
       "      Air Temp-Celsius  Air Temp (?F) Time (24:00) Field_Tech DateVerified  \\\n",
       "0                  8.0          46.40        11:00        NaN          NaN   \n",
       "1                  2.6          36.68        11:30        NaN          NaN   \n",
       "2                  7.6          45.68         9:45        NaN          NaN   \n",
       "3                  2.7          36.86          NaN        NaN          NaN   \n",
       "4                  0.0          32.00        10:30        NaN          NaN   \n",
       "...                ...            ...          ...        ...          ...   \n",
       "2366               NaN          78.00         9:30    Sue Poe   11/13/2019   \n",
       "2367               NaN          58.00         9:30    Sue Poe   11/13/2019   \n",
       "2368               NaN          49.00         9:20    Sue Poe   11/13/2019   \n",
       "2369               NaN          65.00         9:45    Sue Poe   11/13/2019   \n",
       "2370               NaN          42.00         9:40    Sue Poe   11/13/2019   \n",
       "\n",
       "          WhoVerified  AirTemp (C)  Year  \n",
       "0                 NaN     8.000000  1994  \n",
       "1                 NaN     2.600000  1994  \n",
       "2                 NaN     7.600000  1994  \n",
       "3                 NaN     2.700000  1994  \n",
       "4                 NaN     0.000000  1994  \n",
       "...               ...          ...   ...  \n",
       "2366  Christine Folks    25.555556  2018  \n",
       "2367  Christine Folks    14.444444  2018  \n",
       "2368  Christine Folks     9.444444  2018  \n",
       "2369  Christine Folks    18.333333  2018  \n",
       "2370  Christine Folks     5.555556  2018  \n",
       "\n",
       "[2371 rows x 17 columns]"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**Task 2: Clean the data so that the range of years is restricted to the 20 year period from 1999 to 2018**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site_Id</th>\n",
       "      <th>Unit_Id</th>\n",
       "      <th>Read_Date</th>\n",
       "      <th>Salinity (ppt)</th>\n",
       "      <th>Dissolved Oxygen (mg/L)</th>\n",
       "      <th>pH (standard units)</th>\n",
       "      <th>Secchi Depth (m)</th>\n",
       "      <th>Water Depth (m)</th>\n",
       "      <th>Water Temp (?C)</th>\n",
       "      <th>Air Temp-Celsius</th>\n",
       "      <th>Air Temp (?F)</th>\n",
       "      <th>Time (24:00)</th>\n",
       "      <th>Field_Tech</th>\n",
       "      <th>DateVerified</th>\n",
       "      <th>WhoVerified</th>\n",
       "      <th>AirTemp (C)</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/11/1999</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.96</td>\n",
       "      <td>23:02</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/18/1999</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>8.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.40</td>\n",
       "      <td>9:36</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/8/1999</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.35</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.60</td>\n",
       "      <td>14:24</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.777778</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/15/1999</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.65</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.80</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0:00</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.777778</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/10/1999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.70</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.95</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0:00</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.777778</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/11/2018</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.00</td>\n",
       "      <td>9:30</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>25.555556</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/24/2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.00</td>\n",
       "      <td>9:30</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>14.444444</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/28/2018</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.90</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.90</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.00</td>\n",
       "      <td>9:20</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>9.444444</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/7/2018</td>\n",
       "      <td>1.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.90</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.00</td>\n",
       "      <td>9:45</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>18.333333</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/11/2018</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.00</td>\n",
       "      <td>9:40</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Site_Id Unit_Id   Read_Date  Salinity (ppt)  Dissolved Oxygen (mg/L)  \\\n",
       "345        A     NaN   8/11/1999             3.0                     2.60   \n",
       "346        A     NaN   8/18/1999             3.0                     1.60   \n",
       "347        A     NaN    9/8/1999             1.8                     1.35   \n",
       "348        A     NaN  10/15/1999             0.1                     5.65   \n",
       "349        A     NaN  11/10/1999             0.0                     6.70   \n",
       "...      ...     ...         ...             ...                      ...   \n",
       "2366     Bay     NaN  10/11/2018             1.9                     5.00   \n",
       "2367     Bay     NaN  10/24/2018             0.0                     9.00   \n",
       "2368     Bay     NaN  10/28/2018             0.9                     2.90   \n",
       "2369     Bay     NaN   11/7/2018             1.7                      NaN   \n",
       "2370     Bay     NaN  12/11/2018             0.1                      NaN   \n",
       "\n",
       "      pH (standard units)  Secchi Depth (m)  Water Depth (m)  Water Temp (?C)  \\\n",
       "345                   7.7              0.60             0.60             26.0   \n",
       "346                   8.7               NaN              NaN             31.0   \n",
       "347                   8.2              0.40             1.00             28.0   \n",
       "348                   7.3              0.60             0.80             20.0   \n",
       "349                   7.6              0.65             0.95             18.0   \n",
       "...                   ...               ...              ...              ...   \n",
       "2366                  7.0              4.00             1.20             25.0   \n",
       "2367                  7.0              0.30             0.60             18.0   \n",
       "2368                  7.0              0.40             0.90             13.0   \n",
       "2369                  7.0              0.45             0.90             20.0   \n",
       "2370                  7.0              0.10             0.10             10.0   \n",
       "\n",
       "      Air Temp-Celsius  Air Temp (?F) Time (24:00)    Field_Tech DateVerified  \\\n",
       "345                NaN          80.96        23:02  Not Recorded          NaN   \n",
       "346                NaN          91.40         9:36  Not Recorded          NaN   \n",
       "347                NaN          76.60        14:24  Not Recorded          NaN   \n",
       "348                NaN            NaN         0:00  Not Recorded          NaN   \n",
       "349                NaN            NaN         0:00  Not Recorded          NaN   \n",
       "...                ...            ...          ...           ...          ...   \n",
       "2366               NaN          78.00         9:30       Sue Poe   11/13/2019   \n",
       "2367               NaN          58.00         9:30       Sue Poe   11/13/2019   \n",
       "2368               NaN          49.00         9:20       Sue Poe   11/13/2019   \n",
       "2369               NaN          65.00         9:45       Sue Poe   11/13/2019   \n",
       "2370               NaN          42.00         9:40       Sue Poe   11/13/2019   \n",
       "\n",
       "          WhoVerified  AirTemp (C)  Year  \n",
       "345               NaN    27.200000  1999  \n",
       "346               NaN    33.000000  1999  \n",
       "347               NaN    24.777778  1999  \n",
       "348               NaN   -17.777778  1999  \n",
       "349               NaN   -17.777778  1999  \n",
       "...               ...          ...   ...  \n",
       "2366  Christine Folks    25.555556  2018  \n",
       "2367  Christine Folks    14.444444  2018  \n",
       "2368  Christine Folks     9.444444  2018  \n",
       "2369  Christine Folks    18.333333  2018  \n",
       "2370  Christine Folks     5.555556  2018  \n",
       "\n",
       "[1956 rows x 17 columns]"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.query('Year >= 1999 and Year <= 2018')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**Task 3: Store the entire dataset back into a new CSV file called back_bay_1998-2018_clean.csv.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site_Id</th>\n",
       "      <th>Unit_Id</th>\n",
       "      <th>Read_Date</th>\n",
       "      <th>Salinity (ppt)</th>\n",
       "      <th>Dissolved Oxygen (mg/L)</th>\n",
       "      <th>pH (standard units)</th>\n",
       "      <th>Secchi Depth (m)</th>\n",
       "      <th>Water Depth (m)</th>\n",
       "      <th>Water Temp (?C)</th>\n",
       "      <th>Air Temp-Celsius</th>\n",
       "      <th>Air Temp (?F)</th>\n",
       "      <th>Time (24:00)</th>\n",
       "      <th>Field_Tech</th>\n",
       "      <th>DateVerified</th>\n",
       "      <th>WhoVerified</th>\n",
       "      <th>AirTemp (C)</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/11/1999</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.96</td>\n",
       "      <td>23:02</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/18/1999</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>8.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.40</td>\n",
       "      <td>9:36</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/8/1999</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.35</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.60</td>\n",
       "      <td>14:24</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.777778</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/15/1999</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.65</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.80</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0:00</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.777778</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/10/1999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.70</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.95</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0:00</td>\n",
       "      <td>Not Recorded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-17.777778</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/11/2018</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.00</td>\n",
       "      <td>9:30</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>25.555556</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/24/2018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.00</td>\n",
       "      <td>9:30</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>14.444444</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/28/2018</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.90</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.90</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.00</td>\n",
       "      <td>9:20</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>9.444444</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/7/2018</td>\n",
       "      <td>1.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.90</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.00</td>\n",
       "      <td>9:45</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>18.333333</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>Bay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/11/2018</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.00</td>\n",
       "      <td>9:40</td>\n",
       "      <td>Sue Poe</td>\n",
       "      <td>11/13/2019</td>\n",
       "      <td>Christine Folks</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Site_Id Unit_Id   Read_Date  Salinity (ppt)  Dissolved Oxygen (mg/L)  \\\n",
       "0          A     NaN   8/11/1999             3.0                     2.60   \n",
       "1          A     NaN   8/18/1999             3.0                     1.60   \n",
       "2          A     NaN    9/8/1999             1.8                     1.35   \n",
       "3          A     NaN  10/15/1999             0.1                     5.65   \n",
       "4          A     NaN  11/10/1999             0.0                     6.70   \n",
       "...      ...     ...         ...             ...                      ...   \n",
       "1951     Bay     NaN  10/11/2018             1.9                     5.00   \n",
       "1952     Bay     NaN  10/24/2018             0.0                     9.00   \n",
       "1953     Bay     NaN  10/28/2018             0.9                     2.90   \n",
       "1954     Bay     NaN   11/7/2018             1.7                      NaN   \n",
       "1955     Bay     NaN  12/11/2018             0.1                      NaN   \n",
       "\n",
       "      pH (standard units)  Secchi Depth (m)  Water Depth (m)  Water Temp (?C)  \\\n",
       "0                     7.7              0.60             0.60             26.0   \n",
       "1                     8.7               NaN              NaN             31.0   \n",
       "2                     8.2              0.40             1.00             28.0   \n",
       "3                     7.3              0.60             0.80             20.0   \n",
       "4                     7.6              0.65             0.95             18.0   \n",
       "...                   ...               ...              ...              ...   \n",
       "1951                  7.0              4.00             1.20             25.0   \n",
       "1952                  7.0              0.30             0.60             18.0   \n",
       "1953                  7.0              0.40             0.90             13.0   \n",
       "1954                  7.0              0.45             0.90             20.0   \n",
       "1955                  7.0              0.10             0.10             10.0   \n",
       "\n",
       "      Air Temp-Celsius  Air Temp (?F) Time (24:00)    Field_Tech DateVerified  \\\n",
       "0                  NaN          80.96        23:02  Not Recorded          NaN   \n",
       "1                  NaN          91.40         9:36  Not Recorded          NaN   \n",
       "2                  NaN          76.60        14:24  Not Recorded          NaN   \n",
       "3                  NaN            NaN         0:00  Not Recorded          NaN   \n",
       "4                  NaN            NaN         0:00  Not Recorded          NaN   \n",
       "...                ...            ...          ...           ...          ...   \n",
       "1951               NaN          78.00         9:30       Sue Poe   11/13/2019   \n",
       "1952               NaN          58.00         9:30       Sue Poe   11/13/2019   \n",
       "1953               NaN          49.00         9:20       Sue Poe   11/13/2019   \n",
       "1954               NaN          65.00         9:45       Sue Poe   11/13/2019   \n",
       "1955               NaN          42.00         9:40       Sue Poe   11/13/2019   \n",
       "\n",
       "          WhoVerified  AirTemp (C)  Year  \n",
       "0                 NaN    27.200000  1999  \n",
       "1                 NaN    33.000000  1999  \n",
       "2                 NaN    24.777778  1999  \n",
       "3                 NaN   -17.777778  1999  \n",
       "4                 NaN   -17.777778  1999  \n",
       "...               ...          ...   ...  \n",
       "1951  Christine Folks    25.555556  2018  \n",
       "1952  Christine Folks    14.444444  2018  \n",
       "1953  Christine Folks     9.444444  2018  \n",
       "1954  Christine Folks    18.333333  2018  \n",
       "1955  Christine Folks     5.555556  2018  \n",
       "\n",
       "[1956 rows x 17 columns]"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = df.to_csv(\"/home/jovyan/mcis6273_f21_datamining/homework/hw0/back_bay_1998-2018_clean.csv\",index=False)\n",
    "df_readnew = pd.read_csv(\"/home/jovyan/mcis6273_f21_datamining/homework/hw0/back_bay_1998-2018_clean.csv\")\n",
    "df_readnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**TASK 4:**\n",
    "\n",
    "*1. How many and what are the names of the columns in this dataset?*</br>\n",
    "*2. What is the mean Dissolved Oxygen (mg/L) over the entire dataset?*</br>\n",
    "*3. Which year were the highest number of AirTemp (C) data points collected?*</br>\n",
    "*4. Which year were the least number of AirTemp (C) data points collected?*</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**1. How many and what are the names of the columns in this dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1956 entries, 0 to 1955\n",
      "Data columns (total 17 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Site_Id                  1956 non-null   object \n",
      " 1   Unit_Id                  32 non-null     object \n",
      " 2   Read_Date                1956 non-null   object \n",
      " 3   Salinity (ppt)           1867 non-null   float64\n",
      " 4   Dissolved Oxygen (mg/L)  1150 non-null   float64\n",
      " 5   pH (standard units)      1872 non-null   float64\n",
      " 6   Secchi Depth (m)         1891 non-null   float64\n",
      " 7   Water Depth (m)          1890 non-null   float64\n",
      " 8   Water Temp (?C)          1842 non-null   float64\n",
      " 9   Air Temp-Celsius         46 non-null     float64\n",
      " 10  Air Temp (?F)            1904 non-null   float64\n",
      " 11  Time (24:00)             1905 non-null   object \n",
      " 12  Field_Tech               1956 non-null   object \n",
      " 13  DateVerified             386 non-null    object \n",
      " 14  WhoVerified              386 non-null    object \n",
      " 15  AirTemp (C)              1956 non-null   float64\n",
      " 16  Year                     1956 non-null   int64  \n",
      "dtypes: float64(9), int64(1), object(7)\n",
      "memory usage: 259.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_readnew.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**2. What is the mean Dissolved Oxygen (mg/L) over the entire dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.867704347826082"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_readnew[\"Dissolved Oxygen (mg/L)\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**3. Which year were the highest number of AirTemp (C) data points collected?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Year  size\n",
      "0   1999    45\n",
      "1   2000   109\n",
      "2   2001   115\n",
      "3   2002   108\n",
      "4   2003    98\n",
      "5   2004   119\n",
      "6   2005   107\n",
      "7   2006   118\n",
      "8   2007   114\n",
      "9   2008   117\n",
      "10  2009   120\n",
      "11  2010   109\n",
      "12  2011    97\n",
      "13  2012    88\n",
      "14  2013    91\n",
      "15  2014    87\n",
      "16  2015    88\n",
      "17  2016    59\n",
      "18  2017    85\n",
      "19  2018    82\n",
      "The year in which the highest number of AirTemp (C) data points collected is:2009\n"
     ]
    }
   ],
   "source": [
    "filter1 = df_readnew.query('(`AirTemp (C)`==`AirTemp (C)`)') # Remove Null Values in Air Temp-Celsius column\n",
    "airtemp_year = filter1.loc[:,['Year',\"AirTemp (C)\"] ]\n",
    "Year_count=airtemp_year.groupby(['Year'], as_index = False).size()\n",
    "print (Year_count)\n",
    "Year_highcount = Year_count[Year_count['size']==Year_count['size'].max()]['Year']\n",
    "print (\"The year in which the highest number of AirTemp (C) data points collected is:\" +Year_highcount.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**4. Which year were the least number of AirTemp (C) data points collected?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year in which the least number of AirTemp (C) data points collected is:1999\n"
     ]
    }
   ],
   "source": [
    "filter1 = df_readnew.query('(`AirTemp (C)`==`AirTemp (C)`)') # Remove Null Values in Air Temp-Celsius column\n",
    "airtemp_year = filter1.loc[:,['Year',\"AirTemp (C)\"] ]\n",
    "Year_count=airtemp_year.groupby(['Year'], as_index = False).size()\n",
    "Year_lowcount = Year_count[Year_count['size']==Year_count['size'].min()]['Year']\n",
    "print (\"The year in which the least number of AirTemp (C) data points collected is:\" +Year_lowcount.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 5:**\n",
    "\n",
    "*1. Which sampling location has the highest mean ppt? What is the equivalent ppm?*</br>\n",
    "*2. When looking at the mean ppt, which location would you infer is furthest from the influence of ocean water inflows? (Assume that higher salinity correlates to closer proximity to the ocean.)*</br>\n",
    "*3. Dig a little deeper into #2, and write why there may be some uncertainty in your answer? (hint: certainty is improved by consistency in data)*</br>\n",
    "*4. Use the data to determine the correlation between Salinity (ppt) and pH (standard units). Use the DataFrame.corr(). You just need to report the correlation value.*</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**1. Which sampling location has the highest mean ppt? What is the equivalent ppm?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Site_Id  Salinity (ppt)\n",
      "0       A        0.334135\n",
      "1       B        0.444838\n",
      "2     Bay        0.636067\n",
      "3       C        0.596397\n",
      "4       D        0.103226\n",
      "The sampling location with highest mean ppt: Bay\n",
      "The Equivalent ppm is:6360.671462829737\n"
     ]
    }
   ],
   "source": [
    "ppt_df = df_readnew.loc[:,['Site_Id','Salinity (ppt)'] ]\n",
    "mean=ppt_df.groupby(['Site_Id'], as_index = False).mean()\n",
    "mean_pptmax = mean[mean['Salinity (ppt)']==mean['Salinity (ppt)'].max()]['Site_Id']\n",
    "\n",
    "#ppm=ppt*10000\n",
    "Equiv_ppm = mean['Salinity (ppt)'].max() * 10000 \n",
    "print(mean)\n",
    "print (\"The sampling location with highest mean ppt: \" + mean_pptmax.to_string(index=False))\n",
    "print (\"The Equivalent ppm is:\" +format(Equiv_ppm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**2. When looking at the mean ppt, which location would you infer is furthest from the influence of ocean water inflows? (Assume that higher salinity correlates to closer proximity to the ocean.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n"
     ]
    }
   ],
   "source": [
    "mean_pptmin = mean[mean['Salinity (ppt)']==mean['Salinity (ppt)'].min()]['Site_Id']\n",
    "print (mean_pptmin.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location D seems to have less salinity and so I infer this is furthest from the influence of ocean water inflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**3. Dig a little deeper into #2, and write why there may be some uncertainty in your answer? (hint: certainty is improved by consistency in data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 3.  0.  0.4 1.  nan 2. ]\n"
     ]
    }
   ],
   "source": [
    "ppt_df = df_readnew.loc[:,['Site_Id','Salinity (ppt)'] ]\n",
    "mean=ppt_df.loc[ppt_df['Site_Id'] == 'D']\n",
    "unique_df = mean['Salinity (ppt)'].unique()\n",
    "print (unique_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only limited valid values that contributes to the answer in question 2. The more number of ppt values that are consistent, the better will be the certainity of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "**4. Use the data to determine the correlation between Salinity (ppt) and pH (standard units). Use the DataFrame.corr(). You just need to report the correlation value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2712622411317778\n"
     ]
    }
   ],
   "source": [
    "col_Salinity = df_readnew['Salinity (ppt)']\n",
    "col_pH = df_readnew['pH (standard units)']\n",
    "correlation = col_Salinity.corr(col_pH)\n",
    "print (correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer = 0.2712622411317778"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": "1",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
